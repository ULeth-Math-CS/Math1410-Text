\section{The Vector Space $\mathbb{R}^n$}\label{sec:Rn}
This section introduces a collection of concepts and terminology that are at the heart of the subject of linear algebra. The reader is warned that this section is rather long and abstract. The reader may also be encouraged (or perhaps annoyed?) to know that mastery of these concepts is not essential to succeeding in Math 1410. (If you continue to Math 3410, on the other hand, your academic life will depend on understanding these ideas.) It may be productive to treat this section as a reference chapter, to be referred to whenever concepts such as linear independence or span arise later in the text.

In Section \ref{sec:matrix_arithmetic_1} we mentioned that an $n\times 1$ matrix consisting of a single column is known as a \textit{column vector}, and that column vectors are closely related to the vectors we encountered in Chapter \ref{chapter:vectors} in the cases where $n=2$ or $n=3$. 

For each positive integer $n$, the set of all $n\times 1$ column vectors provides an example of what is known as a \sword{vector space}.\index{vector space} 



\definition{def:spaceRn}{The vector space $\mathbb{R}^n$}{
\index{vector space ! of column vectors}
The space of all $n\times 1$ column vectors of real numbers is denoted by
\[
\mathbb{R}^n = \left\{\left.\bbm x_1\\x_2 \\ \vdots \\x_n\ebm \,\,\right| \, x_1, x_2, \ldots, x_n\in \mathbb{R}\right\}.
\]}

As with the vectors in $\mathbb{R}^2$ and $\mathbb{R}^3$ we encountered in Chapter \ref{chapter:vectors}, we allow the notation $\mathbb{R}^n$ to represent both the \textit{space} of points $(x_1,x_2, \ldots, x_n)$, and the set of vectors defined within that space. Since we can identify any point $P$ with the position vector $\overrightarrow{OP}$, the difference between viewing $\mathbb{R}^n$ as a set of points or as a set of vectors is primarily one of perspective.

When $n\geq 4$ we can no longer visualize vectors in $\mathbb{R}^n$ as we did in Chapter \ref{chapter:vectors}, but we can handle them algebraically exactly as we did before, and we can extend the definitions of Chapter \ref{chapter:vectors} to apply to vectors in $\mathbb{R}^n$.

In particular, we can define the \sword{length} of a vector 
\[
\vec{x}=\bbm x_1\\x_2\\ \vdots\\ x_n\ebm\in\mathbb{R}^n
\] 
by
\[
\len{\vec{x}} = \sqrt{x_1^2+x_2^2+\cdots +x_n^2},
\]
and the dot product of vectors $\vec{x}, \vec{y}\in\mathbb{R}^n$ by
\[
\dotp xy = x_1y_1+x_2y_2+\cdots + x_ny_n.
\]
Having defined the dot product, we can still declare two vectors $\vec x$ and $\vec y$ to be orthogonal if $\dotp xy = 0$, and define the angle between two vectors by requiring that the identity
\[
\dotp xy = \len{\vec{x}}\len{\vec{y}}\cos\theta
\]
remain valid. Using these definitions, along with Theorem \ref{thm:addition_properties}, we can see that all of the properties of vector operations given in Theorem \ref{thm:vector_properties} remain valid in $\mathbb{R}^n$.

\smallskip

\theorem{thm:Rn_properties}{Algebraic properties of $\R^n$}{
The following properties hold for the space $\R^n$ of $n\times 1$ column vectors:
\begin{enumerate}
\item If $\vec v$ and $\vec w$ are vectors in $\mathbb{R}^n$, $\vec{v}+\vec{w}$ is also a vector in $\mathbb{R}^n$.
\item For any vectors $\vec v, \vec w$, $\vec v + \vec w = \vec w+ \vec v$.
\item For any vectors $\vec u, \vec v, \vec w$, $(\vec u + \vec v)+\vec w = \vec u + (\vec v + \vec w)$.
\item For any vector $\vec v$, $\vec v+\vec 0 = \vec v$.
\item For any vector $\vec v$, we can define $-\vec v$ such that $\vec v + (-\vec v) = \vec 0$.
\item If $k$ is a scalar and $\vec v$ is a vector in $\mathbb{R}^n$, then $k\vec v$ is also a vector in $\mathbb{R}^n$.
\item For any vector $\vec v$, $1\cdot \vec v = \vec v$.
\item For any scalars $c, d$ and any vector $\vec v$, $c(d\vec v) = (cd)\vec v)$.
\item For any scalar $c$ and vectors $\vec v$, $\vec w$, $c(\vec v+\vec w) = c\vec v+c\vec w$.
\item For any scalars $c, d$ and vector $\vec v$, $(c+d)\vec v = c\vec v+d\vec v$.
\end{enumerate}
}

\smallskip

\mnote{.5}{Vector spaces are defined in general to be sets on which one can define addition and scalar multiplication satisfying the algebraic properties given in Theorem \ref{thm:Rn_properties}. Examples of vector spaces other than $\R^n$ include the space of $m\times n$ matrices (each choice of $m$ and $n$ gives a different space), and the space of all polynomial functions of a real variable. Students interested in learning more about abstract vector spaces should continue on to Math 3410 after completing this course.}

Then ten properties listed in Theorem \ref{thm:Rn_properties} are known as the \sword{vector space axioms}. Any set of objects satisfying these axioms is known as a \sword{vector space}. There are many interesting examples of vector spaces other than $\mathbb{R}^n$, but we will not study vector spaces in general in this text. 

\subsection*{Linear combinations and span}
One of the key insights of linear algebra is that a space such as $\mathbb{R}^n$, which contains infinitely many objects, can be generated using the operations of addition and scalar multiplication from a finite set of basic objects. We saw in Chapter \ref{chapter:vectors}, for example, that every vector in $\mathbb{R}^3$ can be written in terms of just three basic unit vectors $\veci$, $\vecj$, and $\veck$.

Since addition and scalar multiplication are the main operations of linear algebra, it's not too surprising (if a little unimaginative) that any combination of these operations is called a \sword{linear combination}. 

\mnote{.3}{We already defined linear combinations of matrices, and of course column vectors are a special case, but we repeat the definition in the context of $\R^n$ as a reminder.}

\smallskip

\definition{def:linear_comb_Rn}{Linear combination in $\mathbb{R}^n$}{
A \sword{linear combination}\index{linear combination} in $\mathbb{R}^n$ is any expression of the form
\[
c_1\vec{v}_1+c_2\vec{v_2}+\cdots + c_k\vec{v}_k,
\]
where $c_1,c_2\,\ldots, c_k\in \mathbb{R}$ are scalars, and $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\in\mathbb{R}^n$ are vectors.}

\medskip

\example{ex_lincombRn}{Forming linear combinations}{
Let $\vec{u} = \bbm 2\\-1\\3\ebm, \vec{v} = \bbm -4\\ 6\\ 3\ebm, \vec{w} = \bbm 2\\3\\12\ebm$ be vectors in $\mathbb{R}^3$. Form the following linear combinations:
\begin{multicols}{2}
\begin{enumerate}
\item $3\vec{u}-4\vec{w}$
\item $\vec{u}+\vec{v}-2\vec{w}$
\item $7\vec{v}+3\vec{v}$
\item $3\vec{u}+\vec{v}-\vec{w}$
\end{enumerate}
\end{multicols}}
{\begin{enumerate}
\item To simplify the linear combination, we first take care of the scalar multiplication, and then perform the addition. (We choose to interpret this expression as $3\vec{u}+(-4)\vec{w}$, and multiply by $-4$ in the first step, and add in the second step, rather than multiplying by 4 and then subtracting.)
\[
3\vec{u}-4\vec{w}  = 3\bbm 2\\-1\\3\ebm -4\bbm 2\\3\\12\ebm
 = \bbm 6\\-3\\9\ebm + \bbm -8\\-12\\-48\ebm
 = \bbm -2\\-15\\-39\ebm.
\]

\item We proceed as with the previous problem, this time performing the scalar multiplication of $\vec{w}$ by $-2$ in our heads:
\[
\vec{u}+\vec{v}-2\vec{w}  = \bbm 2\\-1\\3\ebm + \bbm -4\\6\\3\ebm + \bbm -4\\-6\\-24\ebm = \bbm -6\\-1\\-18\ebm.
\]

\item We find
\[
7\vec{u}+3\vec{v} = 7\bbm 2\\-1\\3\ebm+3\bbm -4\\6\\3\ebm = 
\bbm 14\\-7\\21\ebm+\bbm -12\\18\\9\ebm = \bbm 2\\11\\30\ebm.
\]

\item For our last example, we compute
\[
3\vec{u}+\vec{v}-\vec{w} = \bbm 6\\-3\\9\ebm + \bbm -4\\6\\3\ebm - \bbm 2\\3\\12\ebm = \bbm 0\\0\\0\ebm.
\]
\end{enumerate}
}

\medskip

Notice that in the last example above, our linear combination works out to be the zero vector. Let's think about this geometrically for a second: using the ``tip-to-tail'' method for adding vectors and beginning with the tail of $3\vec{u}$ at the origin, if we add the vector $\vec{v}$ at the tip of $3\vec{u}$, and then subtract $\vec{w}$, we end up back at the origin. The vectors $3\vec{u}$, $\vec{v}$, and $\vec{w}$ must therefore lie in the same plane, since they form three sides of a triangle, as depicted in Figure \ref{fig:lindeptriangle}.

\mfigure[width=\marginparwidth]{.25}{Depicting the linear combination in Example \ref{ex_lincombRn}.4}{fig:lindeptriangle}{figures/lineardep}

Viewed another way, notice that we can solve the equation $3\vec{u}+\vec{v}-\vec{w}=\vec{0}$ for $\vec{w}$: we have
\[
\vec{w} = 3\vec{u}+\vec{v}.
\]
What this tells us is that when we're being asked to form linear combinations of the vectors $\vec{u}, \vec{v}$, and $\vec{w}$ in Example \ref{ex_lincombRn}, then vector $\vec{w}$ is redundant. Suppose the vector $\vec{x}$ is an arbitrary linear combination of these vectors; that is,
\[
\vec{x} = a\vec{u}+b\vec{v}+c\vec{w}
\]
for some scalars $a,b,c$. If we plug in $\vec{w} = 3\vec{u}+\vec{v}$, then we get
\begin{align*}
\vec{x} &= a\vec{u}+b\vec{v}+c(3\vec{u}+\vec{v})\\
& = a\vec{u}+b\vec{v}+3c\vec{u}+c\vec{v} \tag*{distribute the scalar}\\
& = (a+3c)\vec{u} + (b+c)\vec{v} \tag*{collect terms.}
\end{align*}
Thus, $\vec{x}$ has been written in terms of $\vec{u}$ and $\vec{v}$ only.

These ideas come up frequently enough in Linear Algebra that they have associated terminology. The definitions that follow seem innocent enough, but their importance to the theory of Linear Algebra cannot be understated.

\smallskip

\definition{def:span}{The span of a set of vectors}{
Let $A = \{\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$ be a set of vectors in $\mathbb{R}^n$. The \sword{span}\index{span} of the vectors in $A$, denoted $\operatorname{span}(A)$ is the set $S$ of all possible linear combinations of the vectors in $A$. That is,
\[
S = \operatorname{span}(A) = \{c_1\vec{v}_1+c_2\vec{v}_2+\cdots + c_k\vec{v}_k \,|\, c_1, c_2, \ldots, c_k \in \mathbb{R}\}.
\]}

\medskip

\example{ex_spanRn}{Describing spans in $\mathbb{R}^3$}{
Let $\vec{u}, \vec{v}, \vec{w}$ be as in Example \ref{ex_lincombRn}. Describe the following spans:
\begin{enumerate}
\item $\operatorname{span}\{\vec{u}\}$
\item $\operatorname{span}\{\vec{u},\vec{v}\}$
\item $\operatorname{span}\{\vec{u}, \vec{v}, \vec{w}\}$
\end{enumerate}}
{\begin{enumerate}
\item As a set, we have
\[
\operatorname{span}\{\vec{u}\} = \left\{\left.t\bbm 2\\-1\\3\ebm \, \right|\, t\in \mathbb{R}\right\},
\]
the set of all scalar multiplies of the vector $\vec{u}$. If we think back to Section \ref{sec:lines}, we can do a bit better with our description. The set $\operatorname{span}\{\vec{u}\}$ consists of all vectors $\bbm x\\y\\z\ebm$ such that
\[
\bbm x\\y\\z\ebm = t\bbm 2\\-1\\3\ebm = \bbm 0\\0\\0\ebm + t\bbm 2\\-1\\3\ebm,
\]
which we recognize as the equation of a line through the origin in $\mathbb{R}^3$ in the direction of the vector $\vec{u}$.

\item Again, as a set we can write
\[
\operatorname{span}\{\vec{u},\vec{v}\} = \left\{\left.s\bbm 2\\-1\\3\ebm+t\bbm -4\\6\\3\ebm \, \right| \, s,t\in \mathbb{R}\right\},
\]
so $\operatorname{span}\{\vec{u},\vec{v}\}$ consists of all vectors of the form $\bbm 2s-4t\\-s+6t\\3s+3t\ebm$, where $s$ and $t$ can be any real numbers. Again, with a bit of thought, we can come up with a geometric description of this set. Consider an arbitrary vector
\[
\vec{x} = s\vec{u}+t\vec{v} \in \operatorname{span}\{\vec{u},\vec{v}\}.
\]
Any such vector can be obtained by moving some distance (measured by the scalar $s$) in the direction of $\vec{u}$, and then moving another distance (measured by the scalar $t$) in the direction of $\vec{v}$. We now have two directions in which to move, and if we haven't forgotten what we learned in Section \ref{sec:planes}, this probably reminds us of the description of a plane.

To see that $\operatorname{span}\{\vec{u},\vec{v}\}$ is indeed a plane, we compute
\[
\vec{n} = \vec{u}\times\vec{v} = \bbm -21\\-18\\8\ebm,
\]
which we know is orthogonal to both $\vec{u}$ and $\vec{v}$. It follows from the properties of the dot product that for any other vector $\vec{x}=s\vec{u}+t\vec{v}$ we have
\[
\dotp nx = \vec{n}\boldsymbol{\cdot}(s\vec{u}+t\vec{v}) = s(\dotp ns) + t(\dotp nv) = s(0)+t(0)=0,
\]
so with $\vec{x} = \bbm x\\y\\z\ebm$, we have
\[
-21x-18y+8z=0,
\]
which is the equation of a plane through the origin.

\item In the discussion following Example \ref{ex_lincombRn} we saw that any vector that can be written as a linear combination of $\vec{u}$, $\vec{v}$, and $\vec{w}$ can be written as a linear combination of $\vec{u}$ and $\vec{v}$ alone. Thus, the span of $\vec{u}, \vec{v}$,  and $\vec{w}$ doesn't contain anything we didn't already have in the span of $\vec{u}$ and $\vec{v}$; that is,
\[
\operatorname{span}\{\vec{u},\vec{v},\vec{w}\}=\operatorname{span}\{\vec{u},\vec{v}\}.
\]
\end{enumerate}

\vskip-1.5\baselineskip
}


\mnote{.5}{For any plane through the origin, the sum of two vectors that lie in that plane (or indeed, any linear combination of vectors in the plane) is again a vector in that plane. We'll see shortly that this is the distinguishing characteristic of any \textit{subspace} of $\mathbb{R}^n$.}

\medskip

\example{ex_spanRn2}{Determining membership in a span}{
Given the vectors $\vec{u} = \bbm 2\\-1\\1\ebm$, $\vec{v} = \bbm 3\\2\\5\ebm$, and $\vec{w} = \bbm -2\\5\\3\ebm$, determine whether or not the following vectors belong to $\operatorname{span}\{\vec{u},\vec{v},\vec{w}\}$:
\begin{multicols}{2}
\begin{enumerate}
\item $\vec{x} = \bbm 3\\6\\9\ebm$
\item $\vec{y} = \bbm 4\\1\\-3\ebm$
\end{enumerate}
\end{multicols}
}
{
We do not yet have a general technique for solving problems of this type. Notice that the question ``Does $\vec{x}$ belong to the span of $\{\vec{u},\vec{v},\vec{w}\}$?'' is equivalent to the question, ``Do there exist scalars $a,b,c$ such that
\[
a\vec{u}+b\vec{v}+c\vec{w}=\vec{x}?
\]
Answering this question amounts to solving a system of linear equations: if we plug in our vectors, we have
\[
a\bbm 2\\-1\\1\ebm + b\bbm 3\\2\\5\ebm + c\bbm -2\\5\\3\ebm = \bbm 2a+3b-2c\\-a+2b+5c\\a+5b+3c\ebm = \bbm 3\\6\\9\ebm.
\]
By definition of the equality of vectors, this amounts to the system of equations
\[\arraycolsep=2pt
\begin{array}{ccccccc}
2a&+&3b&-&2c&=&3\\
-a&+&2b&+&5c&=&6\\
a&+&5b&+&3c&=&9
\end{array}
\]
We will develop systematic techniques for solving such systems in the next Chapter. Until then, is there anything we can say? The very astute reader might notice that the vectors $\vec{u},\vec{v}, \vec{w}$ all have something in common: their third component is the sum of the first two: $1=2+(-1)$ for $\vec{u}$, $5=3+2$ for $\vec{v}$, and $3=-2+5$ for $\vec{w}$. Thus, all three vectors are of the form $\bbm x\\y\\x+y\ebm$. Now, notice what happens if we combine two such vectors:
\[
s\bbm a\\b\\a+b\ebm+t\bbm c\\d\\c+d\ebm = \bbm sa+tc\\sb+td\\s(a+b)+t(c+d)\ebm = \bbm sa+tc\\sb+td\\(sa+tc)+(sb+td)\ebm,
\]
which is another vector of the same form. The same will be true for combinations of three or more such vectors. For the vector $\vec{x}$, we check that $3+6=9$, so $\vec{x}$ has the correct form, and indeed (with a bit of ``guess and check'' work, we find that $a=b=c=1$ works, since $\vec{u}+\vec{v}+\vec{w}=\vec{x}$. Thus, we can conclude that
\[
\vec{x}\in\operatorname{span}\{\vec{u},\vec{v},\vec{w}\}.
\]
For the vector $\vec{y}$, we add the first two components, getting $4+1=5\neq -3$. Since the third component is not the sum of the first two, there is no way that $\vec{y}$ could belong to the span of $\vec{u}$, $\vec{v}$, and $\vec{w}$.
}

\medskip


\subsection*{Linear independence and subspaces}
Notice in Example \ref{ex_spanRn} that the span did not change when we added the vector $\vec{w}$ to the set of spanning vectors. This was probably not too surprising, since we saw that $\vec{w} = 3\vec{u}+\vec{v}$, meaning that $\vec{w}$ is a linear combination of $\vec{u}$ and $\vec{v}$, and thus,
\[
\vec{w}\in\operatorname{span}\{\vec{u},\vec{v}\}.
\]
We don't get anything new when we include the vector $\vec{w}$ since it lies in the plane spanned by $\vec{u}$ and $\vec{v}$. We say that the vector $\vec{w}$ \textit{depends} on $\vec{u}$ and $\vec{v}$, in the same way that the total of a sum depends on the numbers being added. Since this dependence is defined in terms of linear combinations, we say that the vectors $\vec{u},\vec{v},\vec{w}$ are \sword{linearly dependent}.\index{linearly dependent}\index{dependent ! linear} In general, a set of vectors $\vec{v}_1,\ldots, \vec{v}_k$ is linearly dependent of one of the vectors can be written as a linear combination of the others. If this is impossible, we say that the vectors are \sword{linearly independent}.\index{linearly independent}\index{independent ! linear} The formal definition is as follows.

\smallskip

\mnote{.25}{For reasons that become apparent as soon as one begins a discussion of basis and dimension (something we won't really cover in this text), we must consider any set containing the zero vector to be  linearly dependent. If we use Equation \eqref{eq:linearindep} to define linear independence, this is included in the definition: if (for example) $\vec{v}_1=\vec{0}$, then we can let $c_1$ be any real number we want, and $c_1\vec{v}_1=\vec{0}$, so it is possible to find a linear combination where not all of the scalars are zero.}

\definition{def:linearindepRn}{Linear dependence}{We say that a set of vectors
\[
A = \{\vec{v}_1,\vec{v}_2,\ldots, \vec{v}_k\}
\]
in $\mathbb{R}^n$ is \sword{linearly dependent} if $\vec{0}\in A$, or if one of the vectors $\vec{v}_i$ in $A$ can be written as a linear combination of the other vectors in $A$. If the set $A$ is not linearly dependent, we say that it is \sword{linearly independent}.}

\pagebreak

\example{ex_linindepRn}{Determining linear independence}{
Determine whether or not following sets of vectors are linearly independent:
\begin{enumerate}
\item The vectors $\vec{u} = \bbm 2\\-1\\1\ebm$, $\vec{v} = \bbm 3\\2\\5\ebm$, and $\vec{w} = \bbm -2\\5\\3\ebm$ from Example \ref{ex_spanRn2}.
\item The vectors 
\[
\vec{x} = \bbm 2\\-1\\0\ebm, \vec{y} = \bbm -2\\3\\1\ebm, \quad \text{ and } \quad \vec{z} = \bbm 1\\1\\0\ebm.
\]
\end{enumerate}
}
{
Like problems involving span, a general approach to answering questions like these about linear independence will have to wait until we develop methods for solving systems of equations in the next chapter. However, for these two sets of vectors, we can reason our way to an answer.
\begin{enumerate}
\item Here, we noticed that all three vectors satisfy the condition $z=x+y$, if we label their respective components as $x$, $y$, and $z$. But this condition is simply the equation of a plane; namely, $x+y-z=0$. Intuition tells us that any plane can be written as the span of \textit{two} vectors, so we can expect that any one of the three vectors can be written in terms of the other two, and indeed, this is the case. With a bit of guesswork (or by asking a computer), we can determine that
\[
\vec{w} = -\frac{19}{7}\vec{u}+\frac{8}{7}\vec{v},
\]
showing that $\vec{w}$ can be written as a linear combination of $\vec{u}$ and $\vec{v}$, and thus, that our vectors are linearly dependent.

\item Here, we make the useful observation that two of our three vectors have zero as their third component. Since $\vec{x}$ and $\vec{z}$ have third component zero, it is impossible for $\vec{y}$ to be written as a linear combination of $\vec{x}$ and $\vec{z}$, since any such linear combination would still have a zero in the third component. To see that $\vec{x}$ cannot be written in terms of $\vec{y}$ and $\vec{z}$, notice that for any $a$ and $b$,
\[
a\vec{y}+b\vec{z} = \bbm -2a+b\\3a+b\\a\ebm.
\]
If this is to equal $\vec{x}$, then we must have $a=0$, giving us $\vec{x}=b\vec{z}$, but it's clear that $\vec{x}$ is not a scalar multiple of $\vec{z}$. A similar argument shows that $\vec{z}$ cannot be written in terms of $\vec{x}$ and $\vec{y}$, and thus our vectors are linearly independent.
\end{enumerate}
}

\medskip

Another way to characterize linear independence is as follows: suppose we have a linear combination equal to the zero vector:
\begin{equation}\label{eq:linearindep}
c_1\vec{v}_1+c_2\vec{v}_2+\cdots +c_k\vec{v}_k = \vec{0}.
\end{equation}
This is always possible of course, since we can simply set each of the scalars equal to zero. The condition of linear independence tells us that if our vectors are independent, then this is the \textbf{only} way to obtain a linear combination equal to the zero vector. 

To see why this rule works, suppose we can choose our scalars in Equation \eqref{eq:linearindep} so that at least one of them is non-zero. For simplicity, let's say $c_1\neq 0$. Then we can rewrite \eqref{eq:linearindep} as
\[
c_1\vec{v}_1 = -c_2\vec{v}_2-\cdots - c_k\vec{v}_k,
\]
and since $c_1\neq 0$, we can divide both sides by $c_1$, and we've written $\vec{v}_1$ as a linear combination of the remaining vectors.

For example, from Example \ref{ex_linindepRn} we can conclude (with a bit of rearranging) that the vectors $\vec{u}$, $\vec{v}$, and $\vec{w}$ satisfy the relationship
\[
19\vec{u}-8\vec{v}+7\vec{w} = \vec{0}.
\]



Linear independence can be a difficult concept at first, but in three dimensions we can use Equation \eqref{eq:linearindep} to provide a visual interpretation on a case-by-case basis. 

\smallskip

\keyidea{idea:indepvect}{Linearly independent sets of vectors in $\mathbb{R}^3$}{
\begin{itemize}
\item Any set $\{\vec{u}\}$ containing a single vector in $\mathbb{R}^3$ is linearly dependent if $\vec{u}=\vec{0}$, and independent otherwise. (Here \eqref{eq:linearindep} becomes $c\vec{u}=\vec{0}$. If $\vec{u}\neq\vec{0}$, the only solution is to take $c=0$.)

\item Any set $\{\vec{u},\vec{v}\}$ containing two non-zero vectors in $\mathbb{R}^3$ is linearly dependent if $\vec{u}$ is parallel to $\vec{v}$, and independent otherwise. (In other words, two \textit{dependent} vectors lie on the same line. Two independent vectors span a plane.)

\item Any set $\{\vec{u},\vec{v},\vec{w}\}$ of three non-zero vectors in $\mathbb{R}^3$ is linearly dependent if all three vectors lie in the same plane, and independent otherwise.

\item Any set of four or more vectors in $\mathbb{R}^3$ is automatically linearly dependent.
\end{itemize}
}

\smallskip

\mnote{.6}{At this point in the text, we're not in a position to prove that any set of four vectors in $\mathbb{R}^3$ (or more generally, $k$ vectors in $\mathbb{R}^n$, where $k>n$) is automatically independent. However, we'll soon see in Chapter \ref{chapter:linear} that in this case, the test for independence given by Equation \eqref{eq:linearindep} results in a ``homogeneous'' system of linear equations with more variables than equations, and that such a system is guaranteed to have non-trivial solutions.}

One of the things we noted in Example \ref{ex_spanRn} was that since $\vec{w}$ belonged to $\operatorname{span}\{\vec{u},\vec{v}\}$, adding $\vec{w}$ to any vector in $\operatorname{span}\{\vec{u},\vec{v}\}$ resulted in another vector in $\operatorname{span}\{\vec{u},\vec{v}\}$. This leads to the notion of a \sword{subspace}, another one of the key concepts in linear algebra.

What sets subspaces apart from other subsets of $\mathbb{R}^n$ is the requirement that all of the properties listed in Theorem \ref{thm:Rn_properties} remain valid when applied to vectors from that subspace. We will not prove it here, but it suffices that the subspace be \textit{closed} under the operations of addition and scalar multiplication.

\smallskip

\definition{def:subspace}{Subspace of $\mathbb{R}^n$}{
A subset $V\subseteq \mathbb{R}^n$ is called a \sword{subspace}\index{subspace} of $\mathbb{R}^n$, provided that the following conditions hold:
\begin{enumerate}
\item For any vectors $\vec u, \vec v\in V$, $\vec u + \vec v \in V$
\item For any vector $\vec v \in V$ and scalar $c\in \mathbb{R}$, $c\vec{v}\in V$.
\end{enumerate}}

\smallskip

\mnote{.3}{Another way of phrasing Definition \ref{def:subspace} is to say that $V$ is a subspace if any linear combination of vectors in $V$ is another vector in $V$; that is, $V$ is closed under taking linear combinations.}

\mnote{.18}{A subspace of $\mathbb{R}^n$ is a subset that looks like a copy of $\mathbb{R}^m$, where $m\leq n$. The visual examples you should keep in mind are lines (which look like copies of $\mathbb{R}$) and planes (which look like copies of $\mathbb{R}^2$) in $\mathbb{R}^3$. However, not all lines and planes will do: as we will see, only those lines and planes that pass through the origin form subspaces.}

It follows from Definition \ref{def:subspace} that any linear combination of vectors in a subspace $V$ is again an element of that subspace. One other important consequence of Definition \ref{def:subspace} must be noted here: since any subspace $V$ is closed under scalar multiplication by \textit{any} scalar, and since $0\cdot\vec v = \vec 0$ for any vector $\vec v$, \textbf{every subspace contains the zero vector}. This often provides an easy test when we want to rule out the possibility that a subset is a subspace.

\medskip

\example{ex_subspace1}{Identifying subspaces}{
Determine which of the following subsets of $\mathbb{R}^3$ are subspaces:
\begin{multicols}{2}
\begin{enumerate}
\item $R = \left\{\left. \bbm x\\y\\z\ebm \, \right|\, 2x-4y+3z=0\right\}$
\item $S = \left\{\left.\bbm x\\4x\\3\ebm \, \right| \, x\in \mathbb{R}\right\}$
\end{enumerate}
\end{multicols}}
{\begin{enumerate}
\item From our work in Section \ref{sec:planes}, we notice right away that the set $R$ describes a plane with normal vector given by $\vec n = \bbm 2\\-4\\3\ebm$. Moreover, this particular plane passes through the origin, since $2(0)-4(0)+3(0)=0$, telling us that $\vec 0 \in R$.

It turns out that any plane through the origin is a subspace, and $R$ is no exception, but let's verify this directly using Definition \ref{def:subspace}. Suppose 
\[
\vec u = \bbm u_1\\u_2\\u_3\ebm \quad \text{ and } \quad \vec v = \bbm v_1\\v_2\\v_3\ebm
\]
are vectors in $R$, so that $2u_1-4u_2+3u_3=0$ and $2v_1-4v_2+3v_3=0$. For the vector
\[
\vec u + \vec v = \bbm u_1+v_1\\u_2+v_2\\u_3+v_3\ebm,
\]
we find
\[
2(u_1+v_1)-4(u_2+v_2)+3(u_3+v_3) = (2u_1-4u_2+3u_3) + (2v_1-4v_2+3v_3) = 0 + 0 = 0,
\]
which shows that $\vec u+\vec v\in R$. Similarly, for any scalar $c$,
\[
2(cu_1)-4(cu_2)+3(cu_3) = c(2u_1-4u_2+3u_3) = 0,
\]
verifying that $c\vec u \in R$. Since $R$ is closed under both addition and scalar multiplication, it is a subspace.

\item For the subset $S$, we immediately notice that the third component must always equal 3; therefore, it is impossible for the zero vector to belong to $S$, and thus $S$ is not a subspace.
\end{enumerate}}

\medskip

To make sure we've got the hang of things, we'll try a couple more examples.

\pagebreak

\example{ex_subspace2}{Identifying subspaces}{
Determine which of the following subsets of $\mathbb{R}^3$ are subspaces:
\begin{multicols}{2}
\begin{enumerate}
\item $T = \left\{\left.\bbm 3a-2b\\a+b\\a-4b\ebm \, \right| \, a,b\in \mathbb{R}\right\}$
\item $U = \left\{\left.\bbm x+y\\3xy\\x^2\ebm \, \right| \, x,y\in \mathbb{R}\right\}$
\end{enumerate}
\end{multicols}}
{\begin{enumerate}
\item Suppose we have two vectors $\vec v = \bbm 3a-2b\\a+b\\a-4b\ebm$ and $\vec w = \bbm 3c-2d\\c+d\\a-4b\ebm$ in the subset $T$. Then we find
\begin{align*}
\vec v + \vec w &= \bbm 3a-2b\\a+b\\a-4b\ebm + \bbm 3c-2d\\c+d\\a-4b\ebm = \bbm (3a-2b)+(3c-2d)\\(a+b)+(c+d)\\(a-4b)+(c-4d)\ebm\\
& = \bbm 3(a+c)-2(b+d)\\(a+c)+(b+d)\\(a+c)-4(b+d)\ebm.
\end{align*}
Since $a+c$ and $b+d$ are again real numbers, we see that $\vec v+\vec w$ fits the definition of $T$, so $\vec v+\vec w\in T$.

Next, if $k\in \mathbb{R}$ is a scalar, we have
\[
k\vec v = k\bbm 3a-2b\\a+b\\a-4b\ebm = \bbm k(3a-2b)\\k(a+b)\\k(a-4b)\ebm = \bbm 3(ka)-2(kb)\\(ka)+(kb)\\(ka)-4(kb)\ebm,
\]
which again fits the patter for vectors in $T$, so $k\vec v\in T$. From Definition \ref{def:subspace}, we can conclude that $T$ is a subspace.

\item You probably recall from your high school mathematics that a function such as $f(x)=ax+b$ is considered \textit{linear}, since its graph is a straight line. Functions like $f(x)=x^2$ are considered non-linear, since their graphs are curved. One of the morals a student does well to learn quickly in \textit{linear} algebra is that any expressions involving non-linear functions of any variables present are not going to play well with the rules of linear algebra.

For the subset $U$, the expressions $3xy$ and $x^2$ tip us off that we are probably not dealing with a subspace here. The easiest way to make sure of this is to check the rules in Definition \ref{def:subspace} using \textit{specific} vectors.

\mnote{.3}{\textbf{Note:} When we want to show that a subset is a subspace, we have to verify that Definition \ref{def:subspace} is satisfied for \textbf{all} possible vectors in that set. Since we generally have infinitely many vectors to deal with, our verification is going to require us to give a general argument, using variables instead of numbers.

On the other hand, if we want to show that a subset is \textbf{not} a subspace, we just have to show that Definition \ref{def:subspace} fails for one or two \textbf{specific} vectors. A choice of vector(s) for which the definition fails is called a \textbf{counterexample}. When constructing a counterexample it's a good idea to choose small numbers to keep the arithmetic simple.}

If we let $x=1$ and $y=2$ in the definition of the set $U$, we get the vector
\[
\vec v = \bbm 1+2\\3(1)(2)\\1^2\ebm = \bbm 3\\6\\1\ebm.
\]
Now consider the vector $4\vec v$. We have
\[
4\vec v = 4\bbm 3\\6\\1\ebm = \bbm 12\\6\\4\ebm.
\]
Looking at the definition of the set $U$, we know that if $4\vec v\in U$, then the third component of $\vec v$ tells us $x^2=4$, so $x=\pm 2$. Now, let's look at the other two components. If $x=2$, we must have
\[
2+y = 12 \text{ and } 3(2)(y) = 6.
\]
The first equation tells us that $y=10$, while the second requires $y=1$. Since $10\neq 1$, this is impossible. Similarly, if $x=-2$, then we would have to have $y=14$ looking at the first component, and $y=-1$ from the second. Since this is again impossible, it must be the case that $4\vec v\notin U$. Since $U$ is not closed under scalar multiplication, $U$ is not a subspace.
\end{enumerate}

\vskip-1.5\baselineskip}

\medskip

After seeing a few examples (and a few exercises), the reader can probably develop some intuition for identifying subspaces. To make sure we don't become too reliant on intuition, however, we'll give one more example with two very similar-looking sets, only one of which is a subspace.

\medskip

\example{ex_subspace3}{Identifying subspaces}{
Determine which of the following subsets of $\mathbb{R}^3$ are subspaces:
\begin{multicols}{2}
\begin{enumerate}
\item $V = \left\{\left.\bbm u+2v\\v+4\\u-2\ebm \, \right| \, u,v\in \mathbb{R}\right\}$
\item $W = \left\{\left.\bbm 2u+v\\v+4\\u-2\ebm \, \right| \, u,v\in \mathbb{R}\right\}$
\end{enumerate}
\end{multicols}}
{\begin{enumerate}
\item The expressions $v+4$ and $u-3$ in the definition of $V$ look like the sort of linear functions we see in high school, but we need to keep in mind that in linear algebra the zero vector has an important role in making sure the algebra works properly. In Linear Algebra, among all functions of the form $f(x)=mx+b$, only those with $b=0$ are considered ``linear'': these are the functions whose graphs are lines through the origin.

The first thing we might check is whether or not $\vec 0 \in V$. If we want
\[
\bbm u+2v\\v+4\\u-2\ebm = \bbm 0\\0\\0\ebm,
\]
then clearly we need $v=-4$ and $u=2$ from the second and third components, but $2+2(-4) = -6\neq 0$, so there is no way to obtain the zero vector as an element of $V$, telling us that $V$ is not a subspace.

\item The subset $W$ looks a lot like the subset $V$, so our instinct is probably telling us that $W$ is not a subspace, either. To know for sure, the first thing we might check is whether or not $\vec 0 \in W$. In this case, we see that $\vec 0$ is indeed in there. Setting $u=2$ and $v=-4$, we get the vector
\[
\bbm 2(2)+(-4)\\-4+4\\2-2\ebm = \bbm 0\\0\\0\ebm = \vec 0,
\]
so $\vec 0 \in W$. Let's try the addition test. Setting $u=1$ and $v=0$ gives us the vector
\[
\vec v = \bbm 2(1)+0\\0+4\\1-2\ebm = \bbm 2\\4\\-1\ebm \in W.
\]
Setting $u=0$ and $v=1$ gives us the vector
\[
\vec w = \bbm 2(0)+1\\1+4\\0-2\ebm = \bbm 1\\5\\-2\ebm \in W.
\]
We now check to see whether or not $\vec v + \vec w \in W$. We have
\[
\vec v + \vec w = \bbm 2\\4\\-1\ebm + \bbm 1\\5\\-2\ebm = \bbm 3\\9\\-3\ebm.
\]
If this is an element of $W$, then we must have $v+4=9$ for some $v\in\mathbb{R}$ (looking at the second component) and $u-2=-3$ for some $u\in \mathbb{R}$ (looking at the third component), so $u=-1$ and $v=5$. Putting these values into the first component, we need to have $2(-1)+5=3$, which is true! Does this mean $W$ is a subspace? Not so fast: we only checked addition for one pair of vectors, and we haven't checked scalar multiplication.

\drawexampleline

If we try a few more examples (the reader is encouraged to do so), we find that things keep working out, so we begin to suspect that maybe $W$ really is a subspace. The only way to know for sure is to attempt to verify Definition \ref{def:subspace} with a general proof. Suppose
\[
\vec v = \bbm 2a+b\\b+4\\a-2\ebm \text{ and } \vec w = \bbm 2c+d\\d+4\\c-2\ebm
\]
are arbitrary elements of $W$. Adding these vectors, we get
\[
\vec v + \vec w = \bbm 2(a+c)+(b+d)\\ (b+d)+8\\(a+c)-4\ebm,
\]
which certainly doesn't look like an element of $W$; the constants are all wrong! We have an 8 in the second component instead of a 4, and a $-4$ in the third component instead of a $-2$. (This is why constant terms in the definition of a subset are generally problematic.)

However, with a bit of sleight of hand, things are not as bad as they seem. Let's write the second component as $(b+d+4)+4$, and the third as $(a+c-2)-2$, and let $v=b+d+4$, and $u=a+c-2$. If $\vec v + \vec w$ is an element of $W$, then we're going to need
\[
2(a+c)+(b+d) = 2u+v,
\]
so that $\vec v + \vec w = \bbm 2u+v\\v+4\\u-2\ebm$ fits the definition of $W$. Is this true? Let's check:
\[
2u+v = 2(a+c-2)+(b+d+4) = 2(a+c)-4+(b+d)+4 = 2(a+c)+(b+d).
\]
The extra constants cancel, so addition works! Similarly, we find
\begin{align*}
k\vec v &= \bbm k(2a+b)\\k(b+4)\\k(c-d)\ebm \\
 & = \bbm 2(ka)+(kb)\\(kb)+4k\\(ka)-2k\ebm\\
 & = \bbm 2(ka)+(kb)\\(kb+4k-4)+4\\(ka-2k+2)-2\ebm\\
 & = \bbm 2(ka-2k+2)+(kb+4k-4)\\(kb+4k-4)+4\\(ka-2k+2)-2\ebm,
\end{align*}
which fits the definition of $W$. (Note that in the last equality, things cancel out again:
\[
2(ka-2k+2)+(kb+4k-4) = 2(ka)-4k+4+(kb)+4k-4 = 2(ka)+(kb).
\]
Whew! That wasn't so straightforward. Could we have made our lives a little bit easier? (The answer to this rhetorical question is almost always yes.)

We know that the potential trouble here came from the constant terms, so one option we have is to try burying them. Given the element 
\[
\vec v = \bbm 2u+v\\v+4\\u-2\ebm \in W,
\]
we're under no obligation to stick with the variables $u$ and $v$. Let's try to simplify a bit: if we let $x = u-2$ (so $u=x+2$) and $y = v+4$ (so $v=y-4$), then
\[
2u+v = 2(x+2)+(y-4) = 2x+4+y-4 = 2x+y,
\]
and thus we can write $\vec v = \bbm 2x+y\\y\\x\ebm$, with no more constant terms. In this form it's much easier to verify that $W$ is a subspace.
\end{enumerate}}

\medskip

Let's take a second look at the subspaces $T$ and $W$ from Examples \ref{ex_subspace2} and \ref{ex_subspace3}. Given an element $\vec v = \bbm 3a-2b\\a+b\\a-4b\ebm$ of $T$, we note that
\[
\vec v = \bbm 3a-2b\\a+b\\a-4b\ebm = \bbm 3a\\a\\a\ebm + \bbm -2b\\b\\-4b\ebm = a\bbm 3\\1\\1\ebm + b\bbm -2\\1\\-4\ebm;
\]
in other words, $T$ can be written as the span of the vectors
\[
\bbm 3\\1\\1\ebm \text{ and } \bbm -2\\1\\-4\ebm.
\]
Similarly, if we write $\vec w = \bbm 2x+y\\y\\x\ebm\in W$ for an arbitrary element of $W$ (using our change of variables), we have
\[
\vec w = \bbm 2x\\0\\x\ebm + \bbm y\\y\\0\ebm = x\bbm 2\\0\\1\ebm + y\bbm 1\\1\\0\ebm,
\]
so the subspace $W$ again can be rewritten as
\[
W = \operatorname{span}\left\{\bbm 2\\0\\1\ebm, \bbm 1\\1\\0\ebm\right\}.
\]

In fact, although we will not prove it in this textbook, \textit{every} subspace of $\mathbb{R}^n$ can be written as the span of some finite set of vectors. (This is usually done in Math 3410.) We can, however, prove that every span is a subspace.

\smallskip

\theorem{thm:subspan}{Every span is a subspace}{\index{subspace ! span}
Let $\vec{v}_1,\vec{v}_2,\ldots, \vec{v}_k$ be vectors in $\mathbb{R}^n$. Then $V=\operatorname{span}\{\vec{v}_1, \vec{v}_2,\ldots, \vec{v}_k\}$ is a subspace of $\mathbb{R}^n$.}

\smallskip

To see that this is true, recall that any element of a span is by definition a linear combination. Given two arbitrary elements
\begin{align*}
\vec a & = a_1\vec{v}_1+a_2\vec{v}_2+\cdots + a_k\vec{v}_k\\
\vec b & = b_1\vec{v}_1+b_2\vec{v}_2+\cdots + b_k\vec{v}_k
\end{align*}
of $V$, we note that
\begin{align*}
\vec a + \vec b & = (a_1\vec{v}_1+a_2\vec{v}_2+\cdots + a_k\vec{v}_k) + (b_1\vec{v}_1+b_2\vec{v}_2+\cdots + b_k\vec{v}_k)\\
& = (a_1\vec{v}_1+b_1\vec{v}_1)+(a_2\vec{v}_2+b_2\vec{v}_2)+\cdots + (a_k\vec{v}_k+b_k\vec{v}_k)\\
& = (a_1+b_1)\vec{v}_1+(a_2+b_2)\vec{v}_2 + \cdots + (a_k+b_k)\vec{v}_k,
\end{align*}
so that $\vec a + \vec b$ can be written as a linear combination of the vectors $\vec{v}_i$ and therefore belongs to $V$. Similarly, for any scalar $c$, we have
\begin{align*}
c\vec a & = c(a_1\vec{v}_1+a_2\vec{v}_2+\cdots + a_k\vec{v}_k)\\
 & = (ca_1)\vec{v}_1+(ca_2)\vec{v}_2+\cdots (ca_k)\vec{v}_k,
\end{align*}
so that $c\vec a$ is an element of $V$ as well. Thus, by Definition \ref{def:subspace}, we know that $V$ is a subspace of $\mathbb{R}^n$.

\smallskip

We conclude with a discussion of how Theorem \ref{thm:subspan} and the concept of linear independence allows us to give a complete description of the possible subspaces of $\mathbb{R}^n$. To begin with, we have the simplest possible subspace, the \sword{trivial subspace}\index{subspace ! trivial} 
\[
V_0 = \{\vec 0\}.
\]

\mnote{.4}{The trivial subspace appears at first glance to be an exception to the rule that ``every subspace is a span'', but we can consider it to be the span of the zero vector. In more advanced textbooks where the concept of a basis is discussed, the trivial subspace is often considered to be the ``span'' of the empty set. Since the empty set contains zero vectors, the trivial subspace is said to be zero-dimensional.}

If a subspace $V$ has at least one non-zero vector, let's say $\vec v\in V$, then by definition it must contain every scalar multiple of that vector. Thus, the next simplest type of subspace is given as the span of a single, non-zero vector:
\[
V_1 = \{t\vec v \, | \, t\in\mathbb{R} \text{ and } \vec v \neq \vec 0\}
\]
Of course, there are infinitely many possibilities for $\vec v$, but each choice of $\vec v\neq \vec 0$ leads to a subspace that looks and acts ``the same''. As discussed earlier, we can picture a subspace of this type as a line through the origin.

Next, we could consider a subspace $V_2 = \operatorname{span}\{\vec v, \vec w\}$, with $\vec v, \vec w \neq \vec 0$. There are two possibilities. One is that $\vec v$ and $\vec w$ are parallel, so that the set $\{\vec v,\vec w\}$ is linearly dependent. In this case we can write $\vec w = k\vec v$ for some scalar $k$, and for any scalars $a$ and $b$,
\[
a\vec v+b\vec w = a\vec v + b(k\vec v) = (a+bk)\vec v,
\]
so our subspace $V_2$ is really of the same type as $V_1$. If, however, the vectors $\vec v$ and $\vec w$ are linearly independent, then adding the vector $\vec w$ gives us a second direction to work with, and $V_2$ becomes an object that is strictly larger than $V_1$. In this case, the visualization is that of a \textit{plane} through the origin.

Depending on the size of $n$, this argument continues. If we add a third vector $\vec u$ that is already in the span of $\vec v$ and $\vec w$, then the set $\{\vec u, \vec v,\vec w\}$ is linearly dependent, and the span of this set is the same as what we already had. If, however, $\vec u \notin\operatorname{span}\{\vec v, \vec w\}$, then $\{\vec u, \vec v,\vec w\}$ is linearly independent, and
\[
V_3 = \operatorname{span}\{\vec u, \vec v, \vec w\}
\]
is a strictly larger subspace than $\operatorname{span}\{\vec v, \vec w\}$. We could then look for a fourth vector, and so on. However, in the familiar case of $\mathbb{R}^3$, the process stops at 3.

\smallskip

\keyidea{idea:R3subspace}{Subspaces of $\mathbb{R}^3$}{
There are four different types of subspaces in $\mathbb{R}^3$:
\begin{itemize}
\item The trivial subspace, $V_0 = \{\vec 0\}$. (Zero dimensional)
\item Lines through the origin, of the form
\[
V_1 = \operatorname{span}\{\vec v\},
\]
where $\vec v\neq \vec 0$. (One dimensional)
\item Planes through the origin, of the form
\[
V_2 = \operatorname{span}\{\vec v, \vec w\},
\]
where the vectors $\vec v, \vec w$ are linearly independent. (Two dimensional)
\item The complete space $V_3 = \mathbb{R}^3$. (Three dimensional)
\end{itemize}}

\smallskip

Notice the reference to dimension in Key Idea \ref{idea:R3subspace}. In $\mathbb{R}^3$, we can rely on our intuitive (geometric) understanding of the concept of dimension. A complete understanding of the concept of dimension will have to wait until Math 3410; however, using the concepts in this section, we can make the following definition.

\smallskip

\definition{def:dimension}{Dimension of a subspace}{\index{dimension}
The \sword{dimension} of a subspace $V\subseteq \mathbb{R}^n$ is the \textit{smallest} number of vectors needed to span $V$.}

\smallskip

One could also define dimension as the \textit{largest} number of linearly independent vectors one can choose from a subspace. If $B=\{\vec{v}_1, \ldots, \vec{v}_k\}$ is a set of vectors in a subspace $V$ such that 
\begin{enumerate}
\item $V=\operatorname{span}(B)$, and
\item $B$ is linearly independent,
\end{enumerate}
then we say $B$ is a \sword{basis}\index{basis} for $V$. For example, the set $\{\veci, \vecj, \veck\}$ is a basis for $\mathbb{R}^3$. There are many possible bases for a subspace, but one can prove that the number of vectors in any basis is the same. Once this fact is established, we could alternatively define dimension as the number of vectors in any basis.

\medskip

This section introduced several new ideas. Some, like linear combinations, are straightforward. Others, like span and linear independence, take some getting used to. There remain two very obvious questions to address:
\begin{enumerate}
\item How do we tell whether or not a given vector belongs to the span of a set of vectors?
\item How do we tell if a set of vectors is linearly independent?
\end{enumerate}
It turns out that both questions lead to \sword{systems of linear equations}. As we saw in Example \ref{ex_spanRn2}, we are currently unable to systematically solve such problems.  In Chapters \ref{chapter:linear} and \ref{chapter:algebra} we will develop the techniques needed to systematically solve such systems, at which point we will be able to easily answer questions about linear independence and span.

To see how such systems arise, suppose we want to know whether or not the vector $\vec w = \bbm 2\\-1\\3\\0\ebm \in\mathbb{R}^4$ belongs to the subspace $V = \operatorname{span}\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$, where
\[
\vec{v}_1 = \bbm 0\\2\\-1\\4\ebm, \vec{v}_2 = \bbm 3\\1\\0\\-4\ebm, \vec{v}_3 = \bbm -3\\6\\7\\2\ebm.
\]
By definition, $V$ is the set of all possible linear combinations of the vectors $\vec{v}_1, \vec{v}_2, \vec{v}_3$, so saying that $\vec w \in V$ is the same as saying that we can write $\vec w$ as a linear combination of these vectors. Thus, what we want to know is whether or not there exist scalars $x_1, x_2, x_3$ such that
\[
\vec w = x_1\vec{v}_1 + x_2\vec{v}_2 + x_3\vec{v}_3.
\]
Substituting in the values for our vectors, this gives
\[
x_1\bbm 0\\2\\-1\\4\ebm + x_2\bbm 3\\1\\0\\-4\ebm + x_3\bbm -3\\6\\7\\2\ebm  = \bbm 3x_2-3x_3\\2x_1+x_2+6x_3\\-x_1+7x_3\\4x_1-4x_2+2x_3\ebm = \bbm 2\\-1\\3\\0\ebm .
\]
Since two vectors are equal if and only if each component is equal, the above vector equation leads to the following system of four equations:
\[\arraycolsep=2pt
\begin{array}{ccccccc}
 & &3x_2&-&3x_3&=&2\\
2x_1&+&x_2&+&6x_3&=&-1\\
-x_1& & &+&7x_3&=&3\\
4x_1&-&4x_2&+&2x_3&=&0
\end{array}
\]
Thus, the question ``Is the vector $\vec w$ an element of $V$?'' is equivalent to the question ``Is there a solution to the above system of equations?'' 

Questions about linear independence are similar, but not quite the same. With the above example involving span, what we wanted to know is ``Does a solution exist?'' With linear independence, it is not whether a solution exists that is in doubt, but whether or not that solution is \textit{unique}. For example, suppose we wanted to know if the vectors in our span example above are linearly independent. We would start with the vector equation
\[
x_1\vec{v}_1+x_2\vec{v}_2+x_3\vec{v}_3 = \vec 0,
\]
and ask whether or not there are any solutions other than $x_1=x_2=x_3=0$. This vector equation leads to a system just like the one above, except that the numbers to the right of the $=$ signs would all be zeros. The techniques needed to answer these and other questions will be developed beginning in Chapter \ref{chapter:linear}.

Before moving on to our study of systems of equations, however, we need to take some time to define \sword{matrix multiplication} and explore some of its properties.

\printexercises{exercises/02_07_exercises}
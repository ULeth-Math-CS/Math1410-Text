\section{Properties of the Determinant}\label{sec:determinant_properties}

\asyouread{
%\item T/F: Having the choice to compute the determinant of a  matrix using cofactor expansion along any row or column is most useful when there are lots of zeros in a row or column.
\item Having the choice to compute the determinant of a  matrix using cofactor expansion along any row or column is most useful when there are lots of what in a row or column?
\item Which elementary row operation does not change the determinant of a matrix?
\item Why do mathematicians rarely smile?
\item T/F: When computers are used to compute the determinant of a matrix, cofactor expansion is rarely used.
}

In the previous section we learned how to compute the determinant. In this section we learn some of the properties of the determinant, and this will allow us to compute determinants more easily. In the next section we will see one application of  determinants.

We start with a theorem that gives us more freedom when computing determinants.

\smallskip

\theorem{thm:cofactor_choice}{Cofactor Expansion Along Any Row or Column}{\index{cofactor!expansion}
Let \tta\ be an $n\times n$ matrix. The determinant of \tta\ can be computed using cofactor expansion along any row or column of \tta.
}

\smallskip

\mnote{.4}{\textbf{Note:} Theorem \ref{thm:cofactor_choice} is sometimes called the \textit{Laplace Expansion Theorem}, after Pierre-Simon, Marquis de Laplace (1749-1827), a French mathematician and physicist whose importance and influence rivals that of Newton. The reader may be wondering why we have not included a proof of this result, which is one of the more important computational facts about the determinant. The answer is that in most textbooks we checked that included the proof, the complete details take up some four pages or so, and don't really add all that much to the understanding of what's going on. In a course like Math 1410, we try to stick to proofs that are short and simple, and that teach us something about the mathematics involved.}

We alluded to this fact way back after Example \ref{ex_cofactor}. We had just learned what cofactor expansion was and we practised along the second row and down the third column. Later, we found the determinant of this matrix by computing the cofactor expansion along the first row. In all three cases, we got the number 0. This wasn't a coincidence. The above theorem states that all three expansions were actually computing the determinant.

How does this help us? By giving us freedom to choose any row or column to use for the expansion, we can choose a row or column that looks ``most appealing.'' This usually means ``it has lots of zeros.'' We demonstrate this principle below.

\medskip

\example{ex_determinant_5}{Computing a $4\times 4$ determinant}{
Find the determinant of 
\[
\tta = \bmx{cccc} 1&2&0&9\\2&-3&0&5\\7&2&3&8\\-4&1&0&2\emx.
\]
}
{
Our first reaction may well be ``Oh no! Not another $4\times 4$ determinant!'' However, we can use cofactor expansion along any row or column that we choose. The third column looks great; it has lots of zeros in it. The cofactor expansion along this column is
\begin{align*}
\det(A) & = a_{1,3}C_{1,3} + a_{2,3}C_{2,3} + a_{3,3}C_{3,3}+a_{4,3}C_{4,3} \\
	&= 0\cdot C_{1,3} + 0\cdot C_{2,3} + 3\cdot C_{3,3} + 0\cdot C_{4,3}
\end{align*}

The wonderful thing here is that three of our cofactors are multiplied by 0. We won't bother computing them since they will not contribute to the determinant. Thus 
\begin{align*}
\det(A) & = 3\cdot C_{3,3} \\
& = 3\cdot (-1)^{3+3}\cdot \bdt{ccc}1&2&9\\2&-3&5\\-4&1&2\edt \\
&= 3\cdot(-147) \quad\quad\quad \left(\parbox{160pt}{\centering\scriptsize we computed the determinant of the $3\times 3$ matrix without showing our work; it is $-147$}\right)\\
&= -447
\end{align*}
}

\medskip

Wow. That was a lot simpler than computing all that we did in Example \ref{ex_determinant_4}. Of course, in that example, we didn't really have any shortcuts that we could have employed. Our next example involves a $5\times 5$ determinant. At first, this looks like trouble, until we realize that the matrix is \textit{triangular}. As we'll see, this makes our job much easier.

\medskip

\enlargethispage{2\baselineskip}

\example{ex_determinant_6}{Computing the determinant of a $5\times 5$ (triangular) matrix}{
Find the determinant of 
\[
\tta = \bbm 1&2&3&4&5\\0&6&7&8&9\\0&0&10&11&12\\0&0&0&13&14\\0&0&0&0&15\ebm.
\]
}
{ 
Since we can expand along any row or column, things are not as bad as they might at first seem. In fact, this problem is very easy. 
What row or column should we choose to find the determinant along? There are two obvious choices: the first column or the last row. Both have 4 zeros in them. We choose the first column.  We omit most of the cofactor expansion, since most of it is just 0:
\[
\det(A) = 1\cdot (-1)^{1+1}\cdot\bvm 6&7&8&9\\0&10&11&12\\0&0&13&14\\0&0&0&15\evm.
\]

Similarly, this determinant is not bad to compute; we again choose to use cofactor expansion along the first column. Note: technically, this cofactor expansion is $6\cdot(-1)^{1+1}A_{1,1}$; we are going to drop the $(-1)^{1+1}$ terms from here on out in this example (it will show up a lot...).
\[
\det(A) = 1\cdot 6\cdot\bvm 10&11&12\\0&13&14\\0&0&15\evm.
\]

You can probably can see a trend. We'll finish out the steps without explaining each one.
\begin{align*}
\det(A) &= 1\cdot6\cdot10\cdot\bvm 13&14\\0&15\evm \\
	&= 1\cdot6\cdot10\cdot13\cdot15\\
	&= 11700
\end{align*}
}

\medskip

We see that the final determinant is the product of the diagonal entries. This works for any triangular matrix (and since diagonal matrices are triangular, it works for diagonal matrices as well). This is an important enough idea that we'll put it into a box.

\smallskip

\keyidea{idea:determinant_triangular}{The Determinant of Triangular Matrices}{\index{determinant!of triangular matrices}\index{triangular matrix!determinant} The determinant of a triangular matrix is the product of its diagonal elements.}

\smallskip

It is now again time to start thinking like a mathematician. Remember, mathematicians see something new and often ask ``How does this relate to things I already know?'' So now we ask, ``If we change a matrix in some way, how is its determinant changed?''

The standard way that we change matrices is through elementary row operations. If we perform an elementary row operation on a matrix, how will the determinant of the new matrix compare to the determinant of the original matrix?

Let's experiment first and then we'll officially state what happens.

\medskip

\example{ex_determinant_row_ops}{Row operations and determinants}{
Let 
\[
\tta = \bmx{cc}1&2\\3&4\emx.
\]
Let \ttb\ be formed from \tta\ by doing one of the following elementary row operations:
\begin{enumerate}
\item		$2R_1+R_2\rightarrow R_2$
\item		$5R_1 \rightarrow R_1$
\item		$R_1\leftrightarrow R_2$
\end{enumerate}
Find $\det(A)$ as well as $\det(B)$ for each of the row operations above.
}
{
It is straightforward to compute $\det(A) = -2$.

Let \ttb\ be formed by performing the row operation in 1) on \tta; thus 
\[
\ttb = \bmx{cc} 1&2\\5&8\emx.
\]
It is clear that $\det(B) = -2$, the same as $\det(A)$.

Now let \ttb\ be formed by performing the elementary row operation in 2) on \tta; that is, 
\[
\ttb = \bmx{cc}5&10\\3&4\emx.
\]
We can see that $\det(B) = -10$, which is $5\cdot\det(A)$.

Finally, let \ttb\ be formed by the third row operation given; swap the two rows of \tta. We see that 
\[
\ttb = \bmx{cc}3&4\\1&2\emx
\]
and that $\det(B) = 2$, which is $(-1)\cdot\det(A)$. \
}

\pagebreak

We've seen in the above example that there seems to be a relationship between the determinants of matrices ``before and after'' being changed by elementary row operations. Certainly, one example isn't enough to base a theory on, and we have not proved anything yet. Regardless, the following theorem is true.

\smallskip

\theorem{thm:determinant_row_operations}{The Determinant and Elementary Row Operations}{\index{elementary row operations!and determinants}\index{determinant!and elementary row operations}
Let \tta\ be an $n\times n$ matrix and let \ttb\ be formed by performing one elementary row operation on \tta.
\begin{enumerate}
	\item		If \ttb\ is formed from \tta\ by adding a scalar multiple of one row to another, then $\det(B)=\det(A)$.
%	\item	If a scalar multiple of one row of \tta\ is added to another row, and the latter row is replaced with that sum to form \ttb, then $\det{\ttb} = \det{\tta}$.
	\item	If \ttb\ is formed from \tta\ by multiplying one row of \tta\ by a scalar $k$, then $\det(B) = k\cdot\det(A)$.
	\item	If \ttb\ is formed from \tta\ by interchanging two rows of \tta, then $\det(B) = -\det(A)$.
\end{enumerate}
}

\smallskip

Let's put this theorem to use in a couple of examples.

\medskip

\example{ex_determinant_7}{Using row operations to compute a determinant}{
Let 
\[
\tta = \bmx{ccc}1&2&1\\0&1&1\\1&1&1\emx.
\]
Compute $\det(A)$, then find the determinants of the following matrices by inspection using Theorem \ref{thm:determinant_row_operations}.
\[
\ttb = \bmx{ccc}1&1&1\\1&2&1\\0&1&1\emx\quad \ttc = \bmx{ccc}1&2&1\\0&1&1\\7&7&7\emx\quad \ttd = \bmx{ccc}1&-1&-2\\0&1&1\\1&1&1\emx
\]
}
{
Computing $\det(A)$ by cofactor expansion down the first column or along the second row seems like the best choice, utilizing the one zero in the matrix. We can quickly confirm that $\det(A) = 1$. 

To compute $\det(B)$, notice that the rows of \tta\ were rearranged to form \ttb. There are different ways to describe what happened; saying $R_1\leftrightarrow R_2$ was followed by $R_1\leftrightarrow R_3$ produces \ttb\ from \tta. Since there were \textit{two} row swaps, $\det(B) = (-1)(-1)\det(A) = \det(A) = 1$.\\ 

Notice that \ttc\ is formed from \tta\ by multiplying the third row by 7. Thus $\det(C) = 7\cdot\det(A) = 7$.\\

It takes a little thought, but we can form \ttd\ from \tta\ by the operation $-3R_2+R_1\rightarrow R_1$. This type of elementary row operation does not change determinants, so $\det(D) = \det(A)$.
} 

\pagebreak

\example{ex_det_elem_row_ops_2}{Effect of elementary row operations on the determinant}{
The matrix \ttb\ was formed from \tta\ using the following elementary row operations, though not necessarily in this order. Find $\det(A)$.\\

\begin{minipage}{.5\linewidth}
\[
B = \bmx{ccc}1&2&3\\0&4&5\\0&0&6\emx
\]
\end{minipage}
\begin{minipage}{.5\linewidth}
$2R_1\rightarrow R_1$

$\frac 13 R_3\rightarrow R_3$

$R_1 \leftrightarrow R_2$

$6R_1+R_2 \rightarrow R_2$
\end{minipage}
}
{
It is easy to compute $\det\ttb=24$. In looking at our list of elementary row operations, we see that only the first three have an effect on the determinant. Therefore 
\[
24 = \det(B) = 2\cdot \frac13  \cdot (-1)\cdot \det(A)
\]
and hence 
\[
\det(A) = -36.
\]
} 

\medskip

In the previous example, we may have been tempted to ``rebuild'' \tta\ using the elementary row operations and then computing the determinant. This can be done, but in general it is a bad idea; it takes too much work and it is too easy to make a mistake. 


Let's continue to think like mathematicians; mathematicians tend to remember ``problems'' they've encountered in the past, and when they learn something new, in the backs of their minds they try to apply their new knowledge to solve their old problem. (This is why mathematicians rarely smile: they are remembering their problems)

What ``problem'' did we recently uncover? We stated in the last chapter that even computers could not compute the determinant of large matrices with cofactor expansion. How then can we compute the determinant of large matrices?

We just learned two interesting and useful facts about matrix determinants. First, the determinant of a triangular matrix is easy to compute: just multiply the diagonal elements. Secondly, we know that given any square matrix, we can use elementary row operations to put the matrix in triangular form.  We can then find the determinant of the new matrix (which is easy), and adjust that number by recalling what elementary operations we performed. 

\medskip

\example{ex_det_elem_row_ops_1}{Reducing a determinant to triangular form}{
Find the determinant of \tta\ by first putting \tta\ into a triangular form, where 
\[
\tta = \bmx{ccc} 2&4&-2\\-1&-2&5\\3&2&1\emx.
\]}
{In putting \tta\ into a triangular form, we need not worry about getting leading 1s, but it does tend to make our life easier as we work out a problem by hand. So let's scale the first row by $1/2$:

\begin{center}\begin{tabular}{ccc}
\parbox{70pt}{\centering\small $\frac 12R_1 \rightarrow R_1$}
&$\quad \quad$&
$\bmx{ccc}1&2&-1\\-1&-2&5\\3&2&1\emx.$
\end{tabular}\end{center}

Now let's get 0s below this leading 1:

\begin{center}\begin{tabular}{ccc}
\parbox{70pt}{\centering\small $R_1 + R_2 \rightarrow R_2$

$-3R_1+R_3\rightarrow R_3$}
&$\quad \quad$&
$\bmx{ccc} 1&2&-1\\0&0&4\\0&-4&4\emx.$
\end{tabular}\end{center}

We can finish in one step; by interchanging rows 2 and 3 we'll have our matrix in triangular form.

\begin{center}\begin{tabular}{ccc}
\parbox{70pt}{\centering\small $R_2  \leftrightarrow R_3$}
&$\quad \quad$&
$\bmx{ccc} 1&2&-1\\0&-4&4\\0&0&4\emx.$
\end{tabular}\end{center}

Let's name this last matrix \ttb. The determinant of \ttb\ is easy to compute as it is triangular; $\det(\ttb) = -16$. We can use this to find $\det(\tta)$. 

Recall the steps we used to transform \tta\ into \ttb. They are:

\begin{center}
$\frac 12R_1 \rightarrow R_1$

$R_1 + R_2 \rightarrow R_2$

$-3R_1+R_3\rightarrow R_3$

$R_2  \leftrightarrow R_3$
\end{center}

The first operation multiplied a row of \tta\ by $\frac 12$. This means that the resulting matrix had a determinant that was $\frac12$ the determinant of \tta. 

The next two operations did not affect the determinant at all. The last operation, the row swap, changed the sign. Combining these effects, we know that 
\[
-16 = \det(\ttb)= (-1)\frac12\det(\tta).
\]

Solving for $\det(\tta)$ we have that $\det(\tta)=32$. \
}

\medskip

In practice, we don't need to keep track of operations where we add multiples of one row to another; they simply do not affect the determinant. Also, in practice, these steps are carried out by a computer, and computers don't care about leading 1s. Therefore, row scaling operations are rarely used. The only things to keep track of are row swaps, and even then all we care about are the number of row swaps. An odd number of row swaps means that the original determinant has the opposite sign of the triangular form matrix; an even number of row swaps means they have the same determinant.

If you find yourself needing to compute a determinant by hand (say, on an exam), it's a good idea to keep the following principles in mind:
\begin{enumerate}
\item Stick to row operations of the type $R_i+kR_j\to R_i$ as much as possible: \textit{they don't change the determinant}.
\item Getting all the way to triangular form isn't really necessary. Use row operations of the above type to create as many zeros as possible in one of the columns, and then expand along that column.
\end{enumerate}

\mnote{.3}{\textbf{Note:} If you want to get really fancy, since $\det(A^T)=\det(A)$, and since performing row operations on $A^T$ is the same as performing \textit{column} operations on $A$, you can  also add a multiple of one column to another without changing the determinant!}

To see how these principles work in practice, let's repeat Example \ref{ex_det_elem_row_ops_1}. This time we'll focus on creating zeros, but we won't worry about getting to triangular form. Since adding a multiple of one row to another doesn't change the determinant, we can compute $\det(A)$ with a string of equalities, as follows:
\begin{align*}
\begin{vmatrix}
2&4&-2\\-1&2&5\\3&2&1
\end{vmatrix} &= \begin{vmatrix}
0&0&8\\-1&2&5\\3&2&1
\end{vmatrix} \tag*{(Add $2R_2$ to $R_1$)}\\
& = 0+0+8(-1)^{1+3}\begin{vmatrix}
-1&-2\\3&2
\end{vmatrix} \tag*{(Expand along Row 1)}\\
& = 8((-1)(2)-(-2)(3))\\
& = 8(4)=32.
\end{align*}
Of course, in this case we got lucky and ended up with two zeros in the first row after one row operation. However, had this not been the case, we would have simply done one more row operation ($R_3+3R_2\to R_3$) to create a second zero in the first column, and then done a cofactor expansion along that column.

For larger determinants, we can follow the same routine: create zeros in one column, expand along that column to get a smaller determinant, and repeat.

Let's think some more like a mathematician. How does the determinant work with other matrix operations that we know? Specifically, how does the determinant interact with matrix addition, scalar multiplication, matrix multiplication, the transpose and the trace? We'll again do an example to get an idea of what is going on, then give a theorem to state what is true.

\medskip

\example{ex_determinant_8}{Determinants and matrix operations}{
Let 
\[
\tta = \bmx{cc}1&2\\3&4\emx \ \text{and} \ \ttb = \bmx{cc} 2&1\\ 3&5\emx.
\]
Find the determinants of the matrices \tta, \ttb, $\tta-\ttb$, $3\tta$, \tta\ttb, \ttat, \ttai. Can you find any connections between these values?
}
{
We can quickly compute that $\det(A) = -2$ and $\det(B) = 7$. 

\begin{align*}
	\det(\tta-\ttb) &= \det\left(\bmx{cc}1&2\\3&4\emx-\bmx{cc} 2&1\\ 3&5\emx\right)\\
	&= \bdt{cc}-1&1\\0&-1\edt \\
	&= 1
\end{align*}
It's tough to find a connection between $\det(\tta-\ttb)$, $\det(A)$ and  $\det(B)$.

\begin{align*}
	\det(3\tta) &= \bdt{cc}3&6\\9&12\edt\\
	&= -18
\end{align*}

We can figure this one out; multiplying one row of \tta\ by 3 increases the determinant by a factor of 3; doing it again (and hence multiplying both rows by 3) increases the determinant again by a factor of 3. Therefore $\det(3\tta) = 3\cdot3\cdot\det(A)$, or $3^2\cdot\det(A)$.

\begin{align*}
	\det(\tta\ttb) &= \det\left(\bmx{cc}1&2\\3&4\emx\bmx{cc} 2&1\\ 3&5\emx\right)\\
	&=	\bdt{cc}8&11\\18 & 23\edt\\
	&= -14
\end{align*}

This one seems clear; $\det(\tta\ttb) = \det(A)\det(B)$.

%\drawexampleline{ex_determinant_8}

\begin{align*}
\det(A^T) & = \bdt{cc} 1&3\\2&4\edt\\
&=	-2
\end{align*}

Obviously $\det(A^T) = \det(A)$; is this always going to be the case? If we think about it, we can see that the cofactor expansion along the first \textit{row} of \tta\ will give us the same result as the cofactor expansion along the first \textit{column} of \ttat.



\begin{align*}
\det(A^{-1}) & = \bdt{cc} -2& 1\\ 3/2 & -1/2\edt\\
&=	1-3/2\\
&=-1/2
\end{align*}

It seems as though 
\[
\det(A^{-1}) = \frac{1}{\det(A)}.
\]
}

\medskip

\mnote{.8}{Seeing that expansion along the first row agrees with expansion along the first column can be a bit tricky to think out in your head. Try it with a 3$\times 3$ matrix \tta\ and see how it works. All the $2\times 2$ submatrices that are created in \ttat\ are the transpose of those found in \tta; this doesn't matter since it is easy to see that the determinant isn't affected by the transpose in a $2\times 2$ matrix.}

We now state a few theorems that confirm our conjectures from the previous example. 

\smallskip

\theorem{thm:no_inverse_zero_determinant}{The determinant of a non-invertible matrix}{If an $n\times n$ matrix $A$ is \textbf{not} invertible, then $\det(A)=0$.}

\smallskip

To see that Theorem \ref{thm:no_inverse_zero_determinant} is true, note that if $A$ is not invertible, then the \rref\ $R$ of $A$ must have a row of zeros. Performing a cofactor expansion along this row, we immediately see that $\det(R)=0$. Since $R$ is obtained from $A$ by a series of elementary row operations, we know from Theorem \ref{thm:determinant_row_operations} that $\det(A)$ is a multiple of $\det(R)$, and thus $\det(A)=0$.

It follows from Theorem \ref{thm:no_inverse_zero_determinant} (using the logical principle known as the \textit{contrapositive})  that if $\det(A)\neq 0$, we're guaranteed that $A$ is invertible.

At this point, we naturally should ask whether or not the converse to Theorem \ref{thm:no_inverse_zero_determinant} is true as well: suppose we know $\det(A)=0$. Does that imply that $A$ is not invertible? (Or equivalently, if we know $A$ is invertible, does this imply that $\det(A)\neq 0$?) The answer is yes, but to see this, we first need a more general result.

\smallskip

\theorem{thm:determinant_properties}{Determinant Properties}{\index{determinant!properties}
Let \tta\ and \ttb\ be $n\times n$ matrices and let $k$ be a scalar. The following are true:
\begin{enumerate}
\item		$\det(kA) = k^n\cdot\det(A)$
\item		$\det(A^T) = \det(A)$
\item		$\det(\tta\ttb) = \det(A)\det(B)$
\end{enumerate}
}

\smallskip

\mnote{.35}{Proving that $\det(AB)=\det(A)\det(B)$ is most easily done using what are called \textit{elementary matrices}. An elementary matrix \index{elementary matrix}\index{matrix!elementary} is any matrix we can obtain by doing a \textbf{single} row operation to the identity matrix. For example, performing the row operation $R_3-4R_2\to R_3$ on $I_3$ gives us the elementary matrix
\[
E = \bbm 1&0&0\\0&1&0\\0&-4&1\ebm.
\]
It turns out that multiplying a matrix \textit{on the left} by an elementary matrix is the same as doing the corresponding row operation: if $A$ is any $3\times 3$ matrix, then $EA$ can be obtained from $A$ using the same row operation used to create $E$.

Theorem \ref{thm:determinant_row_operations} then tells us that 
\[
\det(EB) = \det(E)\det(B)
\]
for any matrix $B$ and \textit{elementary} matrix $E$. The rest boils down to two cases: either $\det(A)=0$, in which case $A$ is not invertible, so neither is $AB$, and thus $\det(AB)=0 = 0\det(B)$, or $\det(A)\neq 0$. In the latter case, $A$ is invertible, and it turns out that any invertible matrix can be written as a product of elementary matrices (these are determined by the row operations used to carry $A$ to the identity matrix). We can then prove that $\det(AB)=\det(A)\det(B)$ by applying Theorem \ref{thm:determinant_row_operations} repeatedly.}

From Theorem \ref{thm:determinant_properties}, we see that $\det(AB)=\det(A)\det(B)$ for any matrices $A$ and $B$. What does this tell us in the case of an invertible matrix? Recall that if $A$ is invertible, then we can determine the inverse matrix $A^{-1}$ such that
\[
AA^{-1} = I_n.
\]
Now, the identity matrix is triangular, and all of its diagonal entries are equal to 1, so we immediately see that $\det(I_n) = 1$. Thus, taking the determinant of both sides of the above equation, we have
\[
\det(AA^{-1}) = \det(A)\det(A^{-1})=1.
\]
We have a product of two numbers equal to one, which tells us that neither of these numbers can be zero. (Otherwise, the product would be zero as well.) Thus, if $A$ is invertible, it must be the case that $\det(A)\neq 0$, so a matrix $A$ is invertible if \textit{and only if} $\det(A)\neq 0$.

As an added bonus, we can rearrange the above equation to give us one more property of the determinant:

\smallskip

\theorem{thm:det_and_inverse}{The determinant of an inverse}{
If $A$ is an invertible matrix, then $\det(A)\neq 0$, and
\[
\det(A^{-1}) = \frac{1}{\det(A)}.
\]}

Combining Theorems \ref{thm:no_inverse_zero_determinant} and \ref{thm:det_and_inverse} allows us to add on to our Invertible Matrix Theorem.

\smallskip

\theorem{thm:IMT_det}{Invertible Matrix Theorem}{\index{Invertible Matrix Theorem}
Let \tta\ be an $n\times n$ matrix. The following statements are equivalent.

\begin{list}{(\alph{IMTcount})}{}\usecounter{IMTcount}
\item \tta\ is invertible. \addtocounter{IMTcount}{\value{IMTcount_temp}}
\item $\det(A)\neq 0$.
\end{list}
\setcounter{IMTcount_temp}{\value{IMTcount}}
\addtocounter{IMTcount_temp}{-1}
}

\smallskip

This new addition to the Invertible Matrix Theorem is very useful; we'll refer back to it in Chapter \ref{chapter:eigen} when we discuss eigenvalues.

%We end this section with a shortcut for computing the determinants of $3\times 3$ matrices.\index{determinant!$3\times 3$ shortcut} Consider the matrix \tta:
%\[
%\bmx{ccc} 1&2&3 \\ 4 & 5 & 6\\7&8&9\emx.
%\]
%We can compute its determinant using cofactor expansion as we did in Example \ref{ex_determinant_2}. Once one becomes proficient at this method, computing the determinant of a $3\times3$ isn't all that hard. A method many find easier, though, starts with rewriting the matrix without the brackets, and repeating the first and second columns at the end as shown below.
%\[
%\begin{array}{ccccc} 1&2&3&1&2 \\ 4 & 5 & 6&4&5\\7&8&9&7&8\end{array}
%\]
%In this $3\times 5$ array of numbers, there are 3 full ``upper left to lower right'' diagonals, and 3 full ``upper right to lower left'' diagonals, as shown below with the arrows.

%\btz [baseline=-3pt,>=latex]
%\node at (0,0) {$\begin{array}{ccccc} 1&2&3&1&2 \\ 4 & 5 & 6&4&5\\7&8&9&7&8\end{array}$};
%\draw[->,  thin] (-1,.4) -- (.5,-.8) node[below right] {45};
%\draw[->,  thin] (-.5,.4) -- (1,-.8) node[below right ] {84};
%\draw[->, thin] (0,.4) -- (1.5,-.8) node[below right] {96};
%\draw[->, thin] (0,.4) -- (-1.5,-.8) node[below left] {105};
%\draw[->, thin] (.5,.4) -- (-1,-.8) node[below left ] {48};
%\draw[->, thin] (1,.4) -- (-.5,-.8) node[below left] {72};
%\etz


%The numbers that appear at the ends of each of the arrows are computed by multiplying the numbers found along the arrows. For instance, the 105 comes from multiplying $3\cdot5\cdot7=105$. The determinant is found by adding the numbers on the right, and subtracting the sum of the numbers on the left. That is, 
%\[
%\det{\tta} = (45+84+96) - (105+48+72) = 0.
%\]

%To help remind ourselves of this shortcut, we'll make it into a Key Idea.

%\smallskip

%\keyidea{idea:3by3shortcut}{3 $\times$ 3 Determinant Shortcut}{
%Let \tta\ be a $3\times 3$ matrix. Create a $3\times 5$ array by repeating the first 2 columns and consider the products of the 3 ``right hand'' diagonals and 3 ``left hand'' diagonals as shown previously. Then 
%\begin{align*}
%\det{\tta} &= \text{``(the sum of the right hand numbers)}\\
% &- \text{ (the sum of the left hand numbers)''}.\\
%\end{align*}
%}

%\smallskip

%We'll practice once more in the context of an example.

%\medskip

%\example{ex_det_shortcut}{Computing a determinant using Key Idea \ref{idea:3by3shortcut}}{Find the determinant of \tta\ using the previously described shortcut, where 
%\[
%\tta = \bmx{ccc} 1&3&9\\-2&3&4\\-5&7&2\emx.\]}
%{Rewriting the first 2 columns, drawing the proper diagonals, and multiplying, we get:

%\btz [baseline=-3pt,>=latex]
%\node at (0,0) {$\begin{array}{ccccc} 1&3&9&1&3 \\ -2&3&4&-2&3\\-5&7&2&-5&7\end{array}$};
%\draw[->, thin] (-1,.4) -- (.5,-.8) node[below right] {{\small 6}};
%\draw[->, thin] (-.5,.4) -- (1,-.8) node[shift={(-.1,0)},below right ] {{\small $-60$}};
%\draw[->, thin] (0.1,.4) -- (1.7,-.8) node[below right] {{\small $-126$}};
%\draw[->,thin] (0,.4) -- (-1.5,-.8) node[below left] {{\small $-135$}};
%\draw[->, thin] (.5,.4) -- (-1,-.8) node[below left ] {{\small 28}};
%\draw[->, thin] (1.1,.4) -- (-.5,-.8) node[shift={(.1,0)},below left] {{\small $-12$}};
%\etz

%Summing the numbers on the right and subtracting the sum of the numbers on the left, we get 
%\[
%\det{\tta}  = (6-60-126) - ( -135+28-12) = -61.
%\] \ } 

%\medskip

In the next section we'll see how the determinant can be used to solve systems of linear equations. 

\printexercises{exercises/03_04_exercises}

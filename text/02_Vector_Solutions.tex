

\section{Vector Solutions to Linear Systems}\label{sec:vector_solutions}

\asyouread{
\item	T/F: The equation \ttaxb\ is just another way of writing a system of linear equations.
\item	T/F: In solving \ttaxo, if there are 3 free variables, then the solution will be ``pulled apart'' into 3 vectors.
\item	T/F: A homogeneous system of linear equations is one in which all of the coefficients are 0.
\item	Whether or not the equation \ttaxb\ has a solution depends on an intrinsic property of \underline{\hskip .5in}.
}


%In the previous two chapters, we explored the algebra of matrices and vectors, and learned how to find solutions to systems of linear equations. We've already seen a few connections between the two; in particular, we ended Section \ref{sec:solving_systems} with several examples where we solved problems involving span and linear independence by setting up and solving systems of linear equations.

So far in this chapter, we've learned a systematic method for solving systems of linear equations. Some of the applied examples we considered in the previous section led naturally to systems of equations, and had solutions that were best interpreted in that context. Other examples, such as those involving span and linear independence, were stated in terms of vectors. In this section, we discuss how to write a system of linear equations in terms of vectors and matrices, and express solutions as vectors. 

%Other concepts we have considered, such as the column space and null space of a matrix, also lead to systems of linear equations; however, in these contexts it is more natural to express the solution to the system in terms of vectors. 

Expressing the solutions of linear systems in terms of vectors will give us additional insight into the behaviour of those systems, and provides a stepping-off point for the study of the algebra of matrices. %We have already seen how to add, subtract, and multiply matrices. The reader might naturally wonder if there is also such a thing as matrix division. The short answer is ``no''. The long answer is that for \textit{some} matrices it is possible to define a matrix \sword{inverse} that behaves in much the same way as the reciprocal of a non-zero number.

We have often relied on previous algebra experience to help us understand linear algebra concepts. We do that again here. Consider the equation $ax=b$, where $a=3$ and $b=6$. If we asked one to ``solve for $x$,'' what exactly would we be asking? We would want to find a number, which we call $x$, where $a$ times $x$ gives $b$; in this case, it is a number, when multiplied by 3, returns 6. As long as $a\neq 0$ (what if $a$ does equal zero?), we know that we can multiply both sides of the equation by $\frac{1}{a}$ to get $x = \frac{1}{a}(b) = \frac{b}{a}$.

Consider a general system of linear equations, of the form
\begin{equation}\label{eq:linsys}\arraycolsep=2pt
\begin{array}{ccccccccc}
a_{11}x_1&+&a_{12}x_2&+&\cdots&+&a_{1n}x_n&=&b_1\\
a_{21}x_1&+&a_{22}x_2&+&\cdots&+&a_{2n}x_n&=&b_2\\
 \vdots & & \vdots & & & & \vdots & & \vdots\\
 a_{m1}x_1&+&a_{m2}x_2&+&\cdots&+&a_{mn}x_n&=&b_m\\
\end{array}
\end{equation}
Notice that the information we place into our augmented matrix can be divided into two pieces: the coefficient matrix
\[
A = \bbm a_{11} & a_{12} & \cdots & a_{1n}\\
         a_{21} & a_{22} & \cdots & a_{2n}\\
         \vdots & \vdots & \ddots & \vdots\\
         a_{m1} & a_{m2} & \cdots & a_{mn}
     \ebm
\]
and the column vector
\[
\vec{b} = \bbm b_1\\b_2\\\vdots \\b_m\ebm.
\]
Moreover, instead of writing our solution as a list ($x_1 = , x_2 = \ldots$), we can arrange our variables into a vector
\[
\vec{x} = \bbm x_1\\x_2\\\vdots\\x_n\ebm,
\]
and express our solution as a single vector rather than a list of numbers.

To create an analogy with the single variable equation $ax=b$, we ask: Is there a way to define the product $A\vec{x}$ of a matrix and a column vector in such a way that the system \eqref{eq:linsys} can be written in the form
\[
A\vec{x}=\vec{b}?
\]
Fortunately for us, the answer is yes! Even better, we'll see that the definition we give here turns out to be a special case (and motivating example) for the general definition of matrix multiplication given in Section \ref{sec:matrix_multiplication}.

The definition of the product $A\vec{x}$ is straightforward. We want the result to be a column vector of size $m$ (that is, with $m$ entries), so that we can set it equal to the column vector $\vec{b}$. Furthermore, each entry in $\vec{b}$ is the right-hand side of an equation in \eqref{eq:linsys}, so we want the corresponding entry in $A\vec{x}$ to be the left-hand side. We are immediately forced to adopt the following rule:

\smallskip

\definition{def:Ax}{The product $A\vec{x}$}{
The product of the matrix $A = \bbm a_{11} & a_{12} & \cdots & a_{1n}\\
         a_{21} & a_{22} & \cdots & a_{2n}\\
         \vdots & \vdots & \ddots & \vdots\\
         a_{m1} & a_{m2} & \cdots & a_{mn}
     \ebm$ and the vector $\vec{x} = \bbm x_1\\x_2\\\vdots\\x_n\ebm$ is given by
 \[
 A\vec{x} = \bbm a_{11}x_1+a_{12}x_2+\cdots +a_{1n}x_n\\
                 a_{21}x_1+a_{22}x_2+\cdots +a_{2n}x_n\\
                 \vdots\\
                 a_{m1}x_1+a_{m2}x_2+\cdots +a_{mn}x_n
            \ebm.    
\]}

\smallskip

A few remarks about Definition \ref{def:Ax} are needed here. First, note that the number of columns in the matrix $A$ matches the number of entries in the vector $\vec{x}$, and the number of rows in $A$ matches the number of entries in the vector $\vec{b}$. Moreover,

\begin{quote}
The $i^{\textrm{th}}$ entry in the vector $A\vec{x}$ is obtained by forming the \textbf{dot product} of row $i$ in the matrix $A$ (viewed as a vector in $\R^n$) with the vector $\vec{x}$.
\end{quote} 

That is, since each row of $A$ has $n$ entries, as does the vector $\vec{x}$, we can form the dot product of $\vec{x}$ with any of the rows of $A$. Each such dot product forms the corresponding entry in the vector $A\vec{x}$.
%Now we consider matrix algebra expressions. We'll eventually consider solving equations like $\tta\ttx = \ttb$, where we know what the matrices \tta\ and \ttb\ are and we want to find the matrix \ttx. For now, we'll only consider equations of the type $\tta\vx=\vb$, where we know the matrix \tta\ and the vector \vb. We will want to find what vector \vx\ satisfies this equation; we want to ``solve for \vx.'' 

To help understand what this is asking, we'll consider an example. Let 
\[
\tta = \bmx{ccc}1&1&1\\1&-1&2\\2&0&1\emx, \quad \vb = \bmx{c}2\\-3\\1\emx \quad \text{and} \quad \vx=\bmx{c}x_1\\x_2\\x_3\emx.
\]
(We don't know what \vx\ is, so we have to represent its entries with the variables $x_1$, $x_2$ and $x_3$.) Let's ``solve for \vx,'' given the equation \ttaxb.

We multiply out the left hand side of this equation according to Definition \ref{def:Ax}. The first row of $A$ is the row vector $\vec{a}_1=\bbm 1&1&1\ebm$. Treating this as a column vector, we form the dot product with $\vec{x}$, giving us
\[
\vec{a}_1\boldsymbol{\cdot}\vec{x} = x_1+x_2+x_3.
\]
Similarly, letting $\vec{a}_2$ and $\vec{a}_3$ denote the second and third rows of $A$, respectively, we find
\begin{align*}
\vec{a}_2\boldsymbol{\cdot}\vec{x} &= x_1-x_2+2x_3\\
\vec{a}_3\boldsymbol{\cdot}\vec{x} &= 2x_1+x_3.
\end{align*}
Putting things together, we find that 
\[
\tta\vx = \bmx{c}x_1+x_2+x_3\\x_1-x_2+2x_3\\2x_1+x_3\emx.
\]
Be sure to note that the product is just a vector; it has just one column.

Setting \tta\vx\ is equal to \vb, we have  
\[
\bmx{c}x_1+x_2+x_3\\x_1-x_2+2x_3\\2x_1+x_3\emx = \bmx{c}2\\-3\\1\emx.
\]
Since two vectors are equal only when their corresponding entries are equal, we know 
\begin{align*} 
x_1+x_2+x_3&=2\\
x_1-x_2+2x_3&=-3\\
2x_1+x_3&=1.
\end{align*}

This should look familiar; it is a system of linear equations! Given the matrix-vector equation $\tta\vx=\vb$, we can recognize \tta\ as the coefficient matrix from a linear system and \vb\ as the vector of the constants from the linear system. Given a system of equations, rewriting it in matrix form is equally straightforward. 

To solve a matrix--vector equation (and the corresponding linear system), we simply augment the matrix \tta\ with the vector \vb,  put this matrix into \rref, and interpret the results.

We convert the above linear system into an augmented matrix and find the \rref: 
\[
\bam{3}1&1&1&2\\1&-1&2&-3\\2&0&1&1\eam \quad\overrightarrow{\text{rref}}\quad \bam{3}1&0&0&1\\0&1&0&2\\0&0&1&-1\eam.
\]
This tells us that $x_1=1$, $x_2=2$ and $x_3 = -1$, so 
\[
\vx = \bmx{c}1\\2\\-1\emx.
\]

We should check our work; multiply out \tta\vx\ and verify that we indeed get \vb: 
\[
\bmx{ccc}1&1&1\\1&-1&2\\2&0&1\emx\bmx{c}1\\2\\-1\emx \quad\text{does equal}\quad \bmx{c}2\\-3\\1\emx.
\]

\medskip

%\enlargethispage{2\baselineskip}

\example{ex_vector_solution_1}{Solving a matrix equation}{Solve the equation $\tta\vx=\vb$ for \vx\ where 
\[
\tta = \bmx{ccc}1&2&3\\-1&2&1\\1&1&0\emx \quad \text{and}\quad \bmx{c} 5\\-1\\2\emx.
\]}
{The solution is rather straightforward, even though we did a lot of work before to find the answer. Form the augmented matrix $\bmx{c|c} \tta&\vb\emx$ and interpret its \rref.
\[
\bam{3} 1&2&3&5\\-1&2&1&-1\\1&1&0&2\eam\quad\overrightarrow{\text{rref}}\quad \bam{3}1&0&0&2\\0&1&0&0\\0&0&1&1\eam
\]

In previous sections we were fine stating that the result as %\begin{align*}x_1&=2\\x_2&=0\\x_3&=1,\end{align*} 
\[
x_1=2, \quad x_2=0,\quad x_3=1,
\]
but we were asked to find \vx; therefore, we state the solution as 
\[
\vx = \bmx{c}2\\0\\1\emx.
\]
\vskip -\baselineskip}

\medskip

This probably seems all well and good. While asking one to solve the equation $\tta\vx=\vb$ for \vx\ seems like a new problem, in reality it is just asking that we solve a system of linear equations. Our variables $x_1$, etc., appear not individually but as the entries of our vector \vx. We are simply writing an old problem in a new way.

In line with this new way of writing the problem, we have a new way of writing the solution. Instead of listing, individually, the values of the unknowns, we simply list them as the elements of our vector \vx.

These are important ideas, so we state the basic principle once more: solving the equation $\tta\vx=\vb$ for \vx\ is the same thing as solving a linear system of equations. Equivalently, any system of linear equations can be written in the form $\tta\vx=\vb$ for some matrix \tta\ and vector \vb.

Since these ideas are equivalent, we'll refer to $\tta\vx=\vb$ both as a matrix--vector equation and as a system of linear equations: they are the same thing.

We've seen two examples illustrating this idea so far, and in both cases the linear system had exactly one solution. We know from Theorem \ref{thm:existence_uniqueness} that any linear system has either one solution, infinitely many solutions, or no solution. So how does our new method of writing a solution work with infinitely many solutions and no solutions?

Certainly, if $\tta\vx=\vb$ has no solution, we simply say that the linear system has no solution. There isn't anything special to write. So the only other option to consider is the case where we have infinitely many solutions. We'll learn how to handle these situations through examples.

\medskip

\example{ex_vector_solution_101}{Finding the vector solution to a linear system}{Solve the linear system $\tta\vx=\zero$ for \vx\ and write the solution in vector form, where 
\[
\tta = \bmx{cc} 1&2\\2&4 \emx \quad \text{and} \quad \zero = \bmx{c}0\\0\emx.
\]}
{\mnote{.35}{\textbf{Note:} Our convention is always that $\vec 0$ denotes the vector of the appropriate size whose entries are all zero, so we didn't really need to specify that $\zero = \bmx{c}0\\0\emx,$ but we did just to eliminate any uncertainty.}

To solve this system, put the augmented matrix into \rref, which we do below.

\[
\bam{2} 1&2&0\\2&4&0\eam \quad \arref \quad \bam{2} 1&2&0\\0&0&0\eam
\]

We interpret the \rref\ of this matrix to write the solution as \begin{align*} 
x_1 &= -2t\\
x_2 &= t \text{ is free.}
\end{align*} 
We are not done; we need to write the solution in vector form, for our solution is the vector \vx. Recall that 
\[
\vx = \bmx{c} x_1\\x_2\emx.
\]
From above we know that $x_1 = -2x_2$, where $x_2=t$, so we replace the $x_1$ in \vx\ with $-2t$ and replace $x_2$ by $t$. This gives our solution as 
\[
\vx = \bmx{c} -2t\\t\emx.
\]
Note that we can now pull the $t$ out of the vector (it is just a scalar) and write \vx\ as 
\[
\vx = t\bmx{c}-2\\1\emx.
\]
For convenience, if we set 
\[
\vv = \bmx{c} -2\\1\emx,
\]
then our solution can be simply written as 
\[
\vx = t\vv.
\]

Recall that since our system was consistent and had a free variable, we have infinitely many solutions. This form of the solution highlights this fact; pick any value for $t$ and we get a different solution. 


For instance, by setting $t = -1$, $0$, and $5$, we get the solutions \[
\vx = \bmx{c}2\\-1\emx, \quad \bmx{c}0\\0\emx,\quad \text{ and }\quad \bmx{c} -10\\5\emx,
\]
respectively. 

We should check our work; multiply each of the above vectors by \tta\ to see if we indeed get \zero. Or, we can save ourselves some time and check the general solution. We have
\[
A\vx = A(t\vv) = t(A\vv) = t\bbm 1&2\\2&4\ebm \bbm -2\\1\ebm = t\bbm 0\\0\ebm = \bbm 0\\0\ebm
\]
for \textit{every} value of $t$.

We have officially solved this problem; we have found the solution to $\tta\vx=\zero$ and written it properly. One final thing we will do here is \textit{graph} the solution, using our skills learned in the previous section.

Our solution is 
\[
\vx = x_2\bmx{c}-2\\1\emx.
\]
This means that any scalar multiply of the vector $\vv = \bmx{c} -2\\1\emx$ is a solution; we know how to sketch the scalar multiples of \vv. This is done in Figure \ref{fig:vect_sol_101}. 

\mtable{.25}{The solution, as a line, to $\protect\tta\protect\vx=\protect\zero$ in Example \ref{ex_vector_solution_101}.}{fig:vect_sol_101}{
\begin{center}
\begin{tikzpicture}[x={(.6cm,0)},y={(0,.6cm)}, >=latex]

\drawxlines{-4.5}{4.5}{-4,...,4};
\drawylines{-3.5}{3.5}{-3,...,3};
\draw [<->](-4,2)--(4,-2);
\draw [->,thick] (0,0)--(-2,1) node [above] {\vv};

\end{tikzpicture}
\end{center}
}

Here vector \vv\ is drawn as well as the line that goes through the origin in the direction of \vv. Any vector along this line is a solution. So in some sense, we can say that the solution to $\tta\vx=\zero$ is \textit{a line}. 
}

\medskip

A few comments are in order here. First, matrix equations (or the corresponding system of linear equations) such as the above where the vector on the right-hand side is the zero vector form a special case that is important enough to have its own name: these are known as \sword{homogeneous} systems of equations. The formal definition is as follows.

\smallskip

\definition{def:homogeneous}{Homogeneous Linear System of Equations}{\index{system of linear equations!homogeneous}\index{homogeneous}
A system of linear equations is \textit{homogeneous} if the constants in each equation are zero.\\

Note: a homogeneous system of equations can be written in vector form as $\tta\vx = \zero$.}

\smallskip

The term \textit{homogeneous} comes from two Greek words; \textit{homo} meaning ``same'' and \textit{genus} meaning ``type.'' A homogeneous system of equations is a system in which each equation is of the same type -- all constants are 0. 

Notice that the line $\vx =t\vv$ in the solution of Example \ref{ex_vector_solution_101} above passes through the origin. This is an important characteristic of homogeneous systems: since $A\vec 0 = \vec 0$ for any matrix $A$, we always have (at least) the solution $\vx = \vec 0$. (We'll have more to say about this below.)

%Furthermore, since or set of solutions is a line through the origin, we know from Section \ref{sec:Rn} is a \textit{subspace}. In fact, it is not just any subspace: this is an important object known as the \sword{null space} of a matrix. We will discuss the importance of the null space in Section \ref{sec:lin_trans}, where we will see that the null space of an $m\times n$ matrix $A$ is the subspace
\[
V = \{\vx\in\R^n \, | \, A\vx = \vec 0\}.
\]
%We saw in Section \ref{sec:lin_trans} that the null space of a matrix $A$ gives us useful information about the linear transformation $T(\vx) = A\vx$, but we mentioned at the time that we did not have the tools to compute it.

%At last, we see that determining the null space of a matrix is exactly the same thing as solving the homogeneous system $A\vx = \vec 0$. Let's try another problem like Example \ref{ex_vector_solution_101}, but this time, we'll do so in the context of Section \ref{sec:lin_trans}.

\medskip

\example{ex_vect_sol_102}{Determining the solution of a homogeneous system}{
Determine the solution to the system $A\vx = \vec 0$, where
\[
\tta = \bmx{cc}2 & -3 \\ -2 & 3\emx.
\]
}
{We proceed exactly as we did in Example \ref{ex_vector_solution_101}, by forming the proper augmented matrix and putting it into \rref, which we do below. 
\[
\bam{2} 2&-3&0\\-2 & 3&0\eam \quad \arref \quad \bam{2} 1 & -3/2 & 0 \\0&0&0\eam
\]

We interpret the \rref\ of this matrix to find that 
\begin{align*}
 x_1 &= 3/2 t \\
  x_2 & = t \text{ is free.}
\end{align*}

As before, we can say that $A\vx=\vec 0$ provided that
\[
\vx = \bbm x_1\\x_2\ebm = \bbm \frac{3}{2}t\\ t\ebm = t\bbm \frac{3}{2}\\1\ebm.
\]
If we set 
\[
\vv = \bmx{c} 3/2\\1\emx,
\]
then our solution can be written as 
\[
\left\{t\vv \,|\, t\in\R \text{ and } \vv = \bmx{c} 3/2\\1\emx\right\}
\]

%\drawexampleline%{ex_vect_sol_102}

Again, we have infinitely many solutions to the equation $A\vx  = \vec 0$; any choice of $x_2$ gives us one of these solutions. For instance, picking $x_2=2$ gives the solution 
\[
\vx = \bmx{c} 3\\2\emx.
\]
This is a particularly nice solution, since there are no fractions! In fact, since the parameter $t$ can take on any \textit{real} value, there is nothing preventing us from defining a new parameter $s = t/2$, and then
\[
\vx = t\bbm 3/2 \\1\ebm = t\left(\frac{1}{2}\bbm 3\\2\ebm \right) = \frac{t}{2}\bbm 3\\2\ebm = s\bbm 3\\2\ebm = s\vec w,
\]
where $\vec w = 2\vv$.

As in the previous example, our solutions are multiples of a vector, and hence we can graph this, as done in Figure \ref{fig:vect_sol_102}.

\mtable{.65}{The solution, as a line, to $\protect\tta\protect\vx=\protect\zero$ in Example \ref{ex_vect_sol_102}.}{fig:vect_sol_102}
{
\begin{center}
\begin{tikzpicture}[x={(.6cm,0)},y={(0,.6cm)}, >=latex]

\drawxlines{-3.5}{6}{-3,...,6};
\drawylines{-2.5}{4.5}{-2,...,4};
\draw [<->](-3,-2)--(6,4);
\draw [->,thick] (0,0)--(1.5,1) node [above] {\vv};

\end{tikzpicture}
\end{center}
}


}

\medskip

In the last two examples, we saw that the general solution could be written in the form $\vx = t\vv$ for a vector $\vv$ such that $A\vv =\vec 0$. Such vectors are known as the \sword{basic solutions} to a  homogeneous linear system.

\smallskip

\definition{def:basic_sol}{Basic solution}{\index{solution!basic} \index{basic solution}
Let $A\vx = \vec 0$ be a homogeneous linear system of equations with infinitely many solutions and free variables 
\[
x_{i_1}=t_1, x_{i_2} = t_2, \ldots, x_{i_k} = t_k.
\]
The \sword{basic} solutions to the system $A\vx = \vec 0$ are the vectors $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k$ such that the general solution to the system is given by
\[
\vx = t_1\vec{v}_1 + t_2\vec{v}_2 + \cdots + t_k\vec{v}_k.
\]
}

\smallskip


To help clarify Definition \ref{def:basic_sol}, let's do one more example where we have more than one basic solution.

\medskip

\example{ex_basic_sols}{A homogeneous system with two basic solutions}{
Find the general solution to the homogeneous system $A\vx = \vec 0$, where 
\[
A = \bbm 1&-2&0&4\\3&-1&5&2\\-2&-6&-10&12\ebm.
\]}
{
As usual, to find the basic solutions, we set up the augmented matrix of the system and reduce:
\[
\bam{4}1&-2&0&4&0\\3&-1&5&2&0\\-2&-6&-10&12&0\eam \quad \arref \quad
\bam{4}1&0&2&0&0\\0&1&1&-2&0\\0&0&0&0&0\eam
\]
From the \rref\ of the augmented matrix, we can read off the following general solution:
\begin{align*}
x_1 &= -2s\\
x_2 &= -s+2t\\
x_3 &= s \text{ is free}\\
x_4 &= t \text{ is free}.
\end{align*}
In this case, we have two parameters, so we expect two basic solutions. To find these, we write our solution in vector form:
\[
\vx = \bbm x_1\\x_2\\x_3\\x_4\ebm = \bbm -2s\\-s+2t\\s\\t\ebm = s\bbm -2\\-1\\1\\0\ebm + t\bbm 0\\2\\0\\1\ebm.
\]
From the above, we see that the general solution can be written as $\vx = s\vv +t\vec{w}$, where 
\[
\vv = \bbm -2\\-1\\1\\0\ebm \quad \text{ and } \quad \vec{w} = \bbm 0\\2\\0\\1\ebm
\]
are the basic solutions to $A\vx = \vec 0$. 
}

\medskip


Let's practice finding vector solutions again; this time, we won't solve a system of the form $\tta\vx=\zero$, but instead $\ttaxb$, for some vector $\vb\neq \vec 0$. Such systems are known (unsurprisingly) as \sword{non-homogeneous} systems.

\medskip

\example{ex_vect_sol_103}{A non-homogeneous linear system}{Solve the linear system $\ttaxb$, where 
\[
\tta = \bmx{cc} 1&2\\2&4\emx \quad \text{and} \quad \vb = \bmx{c} 3\\6\emx.
\]
}
{(Note that this is the same matrix \tta\ that we used in Example \ref{ex_vector_solution_101}. This will be important later.)

Our methodology is the same as before; we form the augmented matrix and put it into \rref.

\[
\bam{2} 1&2&3\\2&4&6\eam \quad \arref \quad \bam{2} 1&2&3\\0&0&0\eam
\]

Interpreting this \rref, we find that 
\begin{align*}
 x_1 &= 3-2t\\
 x_2 &=t \text{ is free.}
\end{align*}
Putting things into vector form, we have
\[
\vx = \bmx{c} x_1\\x_2\emx = \bmx{c}3-2t\\t\emx.
\]

%\drawexampleline

This solution is different than what we've seen in the past two examples; we can't simply pull out a $t$ since there is a 3 in the first entry. Using the properties of matrix addition, we can ``pull apart'' this vector and write it as the sum of two vectors: one which contains only constants, and one that contains only terms involving the parameter $t$. We do this below. 
\begin{align*} 
\vx &= \bmx{c}3-2t\\t\emx \\
		&= \bmx{c} 3\\0\emx + \bmx{c} -2t\\t\emx\\
		&= \bmx{c} 3\\0\emx + t\bmx{c}-2\\1\emx.
\end{align*}

%\drawexampleline

Once again, let's give names to the different component vectors of this solution (we are getting near the explanation of why we are doing this). Let 
\[
\vec{x}_p = \bmx{c} 3\\0\emx \quad \text{and} \quad \vv = \bmx{c}-2\\1\emx.
\]
We can then write our solution in the form 
\[
\vx = \vec{x}_p + t\vv.
\]

We still have infinitely many solutions; by picking a value for $t$ we get one of these solutions. For instance, by letting $t= -1$, $0$, or $2$, we get the solutions 
\[
\bmx{c} 5\\-1\emx, \quad \bmx{c} 3\\0\emx \quad \text{and} \quad \bmx{c} -1\\2\emx.
\]

%\drawexampleline

We have officially solved the problem; we have solved the equation \ttaxb\ for \vx\ and have written the solution in vector form. As an additional visual aid, we will graph this solution. 

Each vector in the solution can be written as the sum of two vectors: $\vec{x}_p$ and a multiple of \vv. In Figure \ref{fig:vect_sol_103}, $\vec{x}_p$ is graphed and \vv\ is graphed with its origin starting at the tip of $\vec{x}_p$. Finally, a line is drawn in the direction of \vv\ from the tip of $\vec{x}_p$; any vector pointing to any point on this line is a solution to \ttaxb.

\mtable{.4}{The solution, as a line, to $\protect\tta\protect\vx=\protect\vb$ in Example \ref{ex_vect_sol_103}.}{fig:vect_sol_103}{
\begin{center}
\begin{tikzpicture}[x={(.6cm,0)},y={(0,.6cm)}, >=latex]

\drawxlines{-4.5}{4.5}{-4,...,4};
\drawylines{-2.5}{5.5}{-2,...,5};
\draw [<->](5,-1)--(-5,4);
\draw [->,thick] (0,0)--(3,0) node [below] {$\vec{x}_p$};
\draw [->,thick] (3,0) -- (1,1) node [above] {\vv};
\end{tikzpicture}
\end{center}}

Notice that in this case our line does \textit{not} pass through the origin, so the set of solutions is not a subspace. On the other hand, every solution to the system $A\vec x = \vec b$ can be obtained by adding the vector $\vec{x}_p$ to an element of the set of solutions to the homogeneous system $A\vx = \vec 0$. We'll elaborate on this shortly.
}

\medskip

The previous examples illustrate some important concepts. One is that (at least, when $\vec x\in \R^2$ or $\R^3$) we can visualize the solution to a system of linear equations. Before, when we had infinitely many solutions, we knew we could arbitrarily pick values for our free variables and get different solutions. We knew this to be true, and we even practised it, but the result was not very ``tangible.'' Now, we can view our solution as a vector; by picking different values for our free variables, we see this as multiplying certain important vectors by a scalar which gives a different solution.

\mnote{.2}{\textbf{Note:} Visually, the solutions in Examples \ref{ex_vector_solution_101} and \ref{ex_vect_sol_103} were both lines; from our experience with Section \ref{sec:lines} we know that this makes sense, since the solutions involved a single parameter $t$. The reader can similarly expect that a solution involving two parameters can be visualized as a plane. Such was the case in Example \ref{ex_basic_sols}, except that here, our solution is a plane in $\R^4$, making it somewhat harder to visualize.}


Another important concept that these examples demonstrate comes from the fact that Examples \ref{ex_vector_solution_101} and \ref{ex_vect_sol_103} were only ``slightly different'' and hence had only ``slightly different'' answers. Both solutions had 
\[
t\bmx{c}-2\\1\emx
\]
in them; in Example \ref{ex_vect_sol_103} the solution also had another vector added to this. The addition of the vector $\vec{x}_p$ in Example \ref{ex_vect_sol_103} is needed to account for the fact that we were dealing with a non-homogeneous system of linear equations.

Recall that for a homogeneous system of linear equations, we know that $\vx = \vec 0$ will be a solution, since no matter what the matrix $A$ is, we can be certain that $\tta\zero = \zero$.  This fact is important; the zero vector is \textit{always} a solution to a homogeneous linear system. Therefore a homogeneous system is always consistent; we need only to determine whether we have exactly one solution (just \zero) or infinitely many solutions. This idea is important, so we give it its own box.

\smallskip

\keyidea{idea:homogeneous}{Homogeneous Systems and Consistency}{\index{consistent}\index{system of linear equations!consistent}\index{homogeneous}

All homogeneous linear systems are consistent.}

\smallskip

How do we determine if we have exactly one or infinitely many solutions? Recall Key Idea \ref{idea:consistent}: if the solution has any free variables, then it will have infinitely many solutions. How can we tell if the system has free variables? Form the augmented matrix $\bmx{c|c} \tta & \zero \emx$, put it into \rref, and interpret the result. 

It may seem that we've brought up a new question, ``When does $\tta\vx=\zero$ have exactly one or infinitely many solutions?'' only to answer with ``Look at the \rref\ of \tta\ and interpret the results, just as always.'' Why bring up a new question if the answer is an old one?

While the new question has an old solution, it does lead to a great idea. Let's refresh our memory; earlier we solved two linear systems, 
\[
\tta\vx = \zero \quad \text{and} \quad \tta\vx = \vb
\]
where
\[
\tta = \bmx{cc}1&2\\2&4\emx \quad \text{and} \quad \vb = \bmx{c}3\\6\emx.
\]
The solution to the first system of equations, \ttaxo, is 
\[
\vx = t\bmx{c}-2\\1\emx = t\vv
\]
and the solution to the second set of equations, \ttaxb, is 
\[
\vx = \bmx{c}3\\0\emx + t\bmx{c}-2\\1\emx = \vec{x}_p+t\vv,
\]
for all $t\in\R$, where $\vec{x}_p = \bbm 3\\0\ebm$ and $\vv = \bbm -2\\1\ebm$.

To see why the general solution to \ttaxb\ works, note that
\[
A\vec{x}_p = \bbm 1&2\\2&4\ebm\bbm 3\\0\ebm = \bbm 3\\6\ebm = \vec{b},
\]
so $\vec{x}_p$ is a solution. (The subscript $p$ of ``$\vec{x}_p$'' is used to denote that this vector is a \sword{particular} solution: see Definition \ref{def:particular}.) What about the general solution $\vec x = \vec{x}_p+t\vv$? Recalling that $A\vv=\zero$, we have
\begin{align*}
A\vx & = A(\vec{x}_p+t\vec v) = A\vec{x}_p + t(A\vv)\\
     & = \vec{b}+t(\vec 0) = \vec{b},
\end{align*}
for any value of $t$, so there are infinitely many solutions to our system, one for each $t\in \R$. The whole point is that $\vec{x}_p$  itself is a solution to $\tta\vx = \vb$, and we could find more solutions by adding vectors ``that go to zero'' when multiplied by \tta. 

So we wonder: does this mean that $\tta\vx = \vb$ will have infinitely many solutions? After all, if $\vec{x}_p$ and $\vec{x}_p+\vv$ are both solutions, don't we have infinitely many solutions?

No. If $\tta\vx = \zero$ has exactly one solution, then $\vv = \zero$, and $\vec{x}_p = \vec{x}_p +\vv$; we only have one solution.

So here is the culmination of all of our fun that started a few pages back. %\footnote{but the fun isn't over} 
 If \vv\ is a solution to $\tta\vx=\zero$ and $\vec{x}_p$ is a solution to $\tta\vx=\vb$, then $\vec{x}_p+\vv$ is also a solution to $\tta\vx=\vb$. If $\tta\vx=\zero$ has infinitely many solutions, so does $\tta\vx = \vb$; if $\tta\vx=\zero$ has only one solution, so does $\tta\vx = \vb$. This culminating idea is of course important enough to be stated again.

\smallskip

\keyidea{idea:unique_solutions}{Solutions of Consistent Systems}{\index{homogeneous}\index{consistent}\index{system of linear equations!consistent}\index{solution!unique}\index{solution!infinitely many}

Let $\tta\vx=\vb$ be a consistent system of linear equations. \begin{enumerate} \item If $\tta\vx = \zero$ has exactly one solution $(\vx = \zero)$, then $\tta\vx = \vb$ has exactly one solution. \item	If $\tta\vx =\zero$ has infinitely many solutions, then $\tta\vx=\vb$ has infinitely many solutions. \end{enumerate}}

\smallskip

A key word in the above statement is \textit{consistent}. If $\tta\vx = \vb$ is inconsistent (the linear system has no solution), then it doesn't matter how many solutions $\tta\vx = \zero$ has; $\tta\vx=\vb$ has no solution.

We can elaborate on Key Idea \ref{idea:unique_solutions} above, as well as Key Idea \ref{idea:consistent} from Section \ref{sec:existence} by introducing one more piece of important terminology. By now it is probably clear that the leading 1s in the \rref\ of a matrix play a key role in understanding the system. In fact, it turns out that we can describe all of the different possibilities for a linear system in terms of one number: the number of leading 1s in the \rref\ of a matrix.

\smallskip

\definition{def:rank}{The rank of a matrix}{
The \sword{rank}\index{rank ! in terms of leading 1s} of a matrix $A$ is denoted by $\operatorname{rank}(A)$ and defined as the number of leading 1s in the \rref\ of $A$.
}

\smallskip

Although we do not prove it in this textbook, the \rref\ of any matrix is \textit{unique}; it follows from this fact that the rank of a matrix is a well-defined number. The importance of rank is outlined in the following result.

\smallskip

\theorem{thm:rank_and_sols}{Rank and solution types}{
Let $A$ be an $m\times n$ matrix.  For any linear system \ttaxb\ in $n$ variables, we have the following possibilities:
\begin{enumerate}
\item If $\operatorname{rank}(A) < \operatorname{rank}\bmx{c|c}A&\vec{b}\emx$, then the system \ttaxb\ is inconsistent.
\item If $\operatorname{rank}(A)=\operatorname{rank}\bmx{c|c}A&\vec b\emx = n$ (where $n$ is the number of variables), then the system \ttaxb\ has a unique solution.
\item If $\operatorname{rank}(A)=\operatorname{rank}\bmx{c|c}A&\vec b\emx = <n$, then the system \ttaxb\ has infinitely solutions. Moreover, the general solution to \ttaxb\ will involve $k$ parameters, where
\[
k = n - \operatorname{rank}(A).
\]
\end{enumerate}
}

To understand Item 1 above, note that if
\[
\operatorname{rank}(A)<\operatorname{rank}\bmx{c|c}A&\vec b\emx,
\]
then there must be a leading 1 in the right-hand column of the \rref\ of $\bmx{c|c}A & \vec b\emx$, meaning that we have a row of the form 
\[
\bmx{cccc|c} 0 & 0 & \cdots & 0 & 1\emx,
\]
which is exactly what we expect in a system with no solutions.

Items 2 and 3 in Theorem \ref{thm:rank_and_sols} simply give another way of stating the fact that the free variables are those variables that do \textbf{not} have a leading 1 in their column. This seems like an obvious fact, but it is very important. \ifthenelse{\boolean{colour}}{We will see in Section \ref{sec:null_column} that this observation leads to a major theorem, sometimes known as the Fundamental Theorem of Linear Transformations: see Theorem \ref{thm:fund_thm_lin_maps}.}{Indeed, this observation leads to a major theorem, sometimes known as the Fundamental Theorem of Linear Transformations, or the Rank-Nullity Theorem.}

Let us explore this result with a series of examples.

\medskip


\example{ex_vector_solution_5}{Using matrices and vectors to solve a system of equations}{Rewrite the linear system 
\[
\begin{array}{ccccccccccc} x_1&+&2x_2&-&3x_3&+&2x_4&+&7x_5&=&2\\3x_1&+&4x_2&+&5x_3&+&2x_4&+&3x_5&=&-4 \end{array}
\]
as a matrix--vector equation, solve the system using vector notation, and give the solution to the related homogeneous equations.}
{Rewriting the linear system in the form of $\tta\vx=\vb$, we have that 
\[
\tta = \bmx{ccccc}1&2&-3&2&7\\3&4&5&2&3\emx,\quad \vx = \bmx{c}x_1\\x_2\\x_3\\x_4\\x_5\emx \quad \text{and} \quad \vb = \bmx{c}2\\-4\emx.
\]
To solve the system, we put the associated augmented matrix into \rref\ and interpret the results.


\[
\bam{5}1&2&-3&2&7&2\\3&4&5&2&3&-4\eam\quad \arref \quad \bam{5}1&0&11&-2&-11&-8\\0&1&-7&2&9&5\eam
\]
\begin{align*}
  x_1&=-8-11r+2s+11t\\ 
  x_2&=5+7r-2s-9t\\
  x_3&=r \text{ is free}\\
  x_4&=s \text{ is free}\\
  x_5&=t \text{ is free}
\end{align*}

We use this information to write \vx, again pulling it apart. Since we have three free variables and also constants, we'll need to pull \vx\ apart into four separate vectors.

\begin{align*}
\vx &= \bmx{c}x_1\\x_2\\x_3\\x_4\\x_5\emx = \bmx{c}-8-11r+2s+11t\\5+7r-2s-9t\\r\\s\\t\emx\\
    &= \bmx{c}-8\\5\\0\\0\\0\emx + \bmx{c}-11r\\7r\\r\\0\\0\emx+ \bmx{c}2s\\-2s\\0\\s\\0\emx + \bmx{c}11t\\-9t\\0\\0\\t\emx \\
    &= \bmx{c}-8\\5\\0\\0\\0\emx + r\bmx{c}-11\\7\\1\\0\\0\emx+ s\bmx{c}2\\-2\\0\\1\\0\emx + t\bmx{c}11\\-9\\0\\0\\1\emx \\
    &= \underbrace{\vec{x}_p}_{\parbox{35pt}{\center \scriptsize {\vskip -10pt} particular solution}}+ \underbrace{r\vu+s\vv+t\vect{w}}_{\parbox{100pt}{\center \scriptsize {\vskip -10pt} solution to homogeneous equations $\tta\vx=\zero$}}
\end{align*}


So $\vec{x}_p$ is a particular solution; $\tta\vxp = \vb$. (Multiply it out to verify that this is true.) The other vectors, \vu, \vv\ and \vect{w}, that are multiplied by our free variables $x_3=r$, $x_4=s$ and $x_5=t$, are each solutions to the homogeneous equations, $\tta\vx=\zero$. Any linear combination of these three vectors, i.e., any vector found by choosing values for $r$, $s$ and $t$ in $r\vu+s\vv+t\vect{w}$ is a solution to $\tta\vx=\zero$.
}

\medskip

\example{ex_vector_solution_6}{Finding vector solutions}{Let 
\[
\tta = \bmx{cc}1&2\\4&5\emx \quad \text{and} \quad \vb = \bmx{c} 3\\6\emx.
\]
Find the solutions to $\tta\vx=\vb$ and $\tta\vx = \zero$.}
{We go through the familiar work of finding the \rref\ of the appropriate augmented matrix and interpreting the solution. 
\[
\bmx{cc|c}1&2&3\\4&5&6\emx \quad \arref \quad \bmx{cc|c}1&0&-1\\0&1&2\emx
\]
\begin{align*} 
  x_1 &= -1\\
  x_2 &= 2
\end{align*}

Thus 
\[
\vx = \bmx{c}x_1\\x_2\emx = \bmx{c}-1\\2\emx.
\]

This may strike us as a bit odd; we are used to having lots of different vectors in the solution. However, in this case, the linear system $\tta\vx = \vb$ has exactly one solution, and we've found it. What is the solution to $\tta\vx=\zero$? Since we've only found one solution to $\tta\vx=\vb$, we can conclude from Key Idea \ref{idea:unique_solutions} the related homogeneous equations $\tta\vx=\zero$ have only one solution, namely $\vx = \zero$. We can write our solution vector \vx\ in a form similar to our previous examples to highlight this: 
\begin{align*}
\vx &= \bmx{c}-1\\2\emx \\
    &= \bmx{c}-1\\2\emx + \bmx{c}0\\0\emx\\
    &= \underbrace{\vxp}_{\parbox{35pt}{\scriptsize \center {\vskip -10pt}particular solution}} + \underbrace{\zero_{ \parbox{1pt}{{\vskip 5pt}} }}_{\parbox{35pt}{\center \scriptsize {\vskip -10pt} solution to $\tta\vx=\zero$}}.
\end{align*} 
Again, in light of Theorem \ref{thm:rank_and_sols}, this should not be too surprising. The \rref\ of $A$ is $\bbm 1&0\\0&1\ebm$, so the rank of $A$ is 2, and there are 2 variables in our system, so we expect $2-2=0$ parameters in our general solution.
}

\medskip

\example{ex_vector_solution_7}{Further vector solutions}{Let 
\[
\tta = \bmx{cc}1&1\\2&2\emx \quad \text{and} \quad \vb = \bmx{c} 1\\1\emx.
\]
Find the solutions to $\tta\vx=\vb$ and $\tta\vx = \zero$.}
{To solve \ttaxb, we put the appropriate augmented matrix into \rref\ and interpret the results.

\[
\bam{2}1&1&1\\2&2&1\eam \quad \arref \quad \bam{2}1&1&0\\0&0&1\eam
\]

We immediately have a problem; we see that the second row tells us that $0x_1+0x_2 = 1$, the sign that  our system does not have a solution. Thus \ttaxb\ has no solution. Of course, this does not mean that \ttaxo\ has no solution; it always has a solution.

\mnote{.75}{As previously noted, the fact that \ttaxb\ has no solution in Example \ref{ex_vector_solution_7} simply indicates the fact that $\vec{b}$ is not in the column space of $A$. Since the rank of $A$ is equal to one, we know that $\operatorname{col}(A)$ is spanned by the single vector $\vv = \bbm 1\\2\ebm$. Thus, we can only expect \ttaxb\ to have a solution if $\vb$ is a scalar multiple of $\vv$.}

\enlargethispage{2\baselineskip}

To find the solution to \ttaxo, we interpret the \rref\ of the appropriate augmented matrix. 
\[
\bmx{ccc}1&1&0\\2&2&0\emx \quad \arref \quad \bmx{ccc}1&1&0\\0&0&0\emx
\]
\begin{align*} 
x_1 &=-x_2 \\ x_2 &\text{ is free}
\end{align*} 

Thus 
\[
\vx = \bmx{c} x_1\\x_2\emx = \bmx{c}-x_2\\x_2\emx = x_2\bmx{c}-1\\1\emx 
= x_2\vu.
\]

We have no solution to \ttaxb, but infinitely many solutions to \ttaxo. \vskip-0.5\baselineskip}

\pagebreak

The previous example may seem to violate the principle of Key Idea \ref{idea:unique_solutions}. After all, it seems that having infinitely many solutions to \ttaxo\ should imply infinitely many solutions to \ttaxb. However, we remind ourselves of the key word in the idea that we observed before: \textit{consistent}. If \ttaxb\ is consistent and \ttaxo\ has infinitely many solutions, then so will \ttaxb. But if \ttaxb\ is not consistent, it does not matter how many solutions \ttaxo\ has; \ttaxb\ is still inconsistent.


\medskip

In this chapter, we developed a systematic method for solving systems of linear equations. A key tool in this method was the augmented matrix corresponding to a given system. In this final section, we've seen that further insight into the structure of solutions can be gained by considering our systems in terms of matrices and vectors.

In the next chapter, we will begin the study of matrices as objects unto themselves. We will see that they can be added and multiplied by scalars in exactly the same way as vectors, and in addition to this, matrices of the correct sizes can be \textit{multiplied} in a way that reproduces Definition \ref{def:Ax} above as a special case. 

One question that may have occurred to you as you worked through this section is the following: in the one-variable linear equation $ax=b$, we know that as long as $a\neq 0$, we can divide both sides by $a$, giving us the solution $x=b/a$. Now, given the matrix equation $A\vec{x}=\vec{b}$, is there some equivalent means of ``dividing by $A$'' to obtain the solution $\vec{x}$? The short answer is no. Indeed, there is no such thing as matrix division; the algebraic rules for matrix multiplication are much more complicated than they are for numbers. (In particular, we'll see that for matrices, $AB$ is usually \textbf{not} the same thing as $BA$!) 

The slightly longer answer to our question might be phrased as ``Sometimes. Well, sort of.'' To obtain the correct (and much longer) answer, we will be led in the next chapter to the definition of the \textit{inverse} of a matrix.
%Exercises

%Lots of examples finding homogeneous and particular solutions.\\

%\clearpage

\printexercises{exercises/02_03_exercises}

%\clearpage
%\printanswers{exercises/02_03_exercises}

